{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQ7ScDsSH0OJl9FCHoQvRJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnupriya-Selvraj/Agentic_AI_Workshop/blob/main/RAG_System_AI_Research_Papers_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-huggingface\n",
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install pypdf2 PyPDF2\n",
        "!pip install transformers torch\n",
        "!pip install gradio\n",
        "!pip install accelerate\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ],
      "metadata": {
        "id": "-Y5dODOv3x2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from typing import List, Dict, Any\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üî• GPU Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "uVLxq41332aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"Handles loading and processing of PDF documents\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=1000, chunk_overlap=200):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def load_pdfs(self, pdf_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"Load multiple PDF files and return documents\"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            try:\n",
        "                print(f\"üìñ Loading: {os.path.basename(pdf_path)}\")\n",
        "                loader = PyPDFLoader(pdf_path)\n",
        "                documents = loader.load()\n",
        "\n",
        "                # Add metadata\n",
        "                for i, doc in enumerate(documents):\n",
        "                    doc.metadata.update({\n",
        "                        'source_file': os.path.basename(pdf_path),\n",
        "                        'page_number': i + 1,\n",
        "                        'total_pages': len(documents)\n",
        "                    })\n",
        "\n",
        "                all_documents.extend(documents)\n",
        "                print(f\"‚úÖ Loaded {len(documents)} pages from {os.path.basename(pdf_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading {pdf_path}: {str(e)}\")\n",
        "\n",
        "        return all_documents\n",
        "\n",
        "    def create_chunks(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for better retrieval\"\"\"\n",
        "        print(\"üî™ Creating text chunks...\")\n",
        "\n",
        "        chunks = self.text_splitter.split_documents(documents)\n",
        "\n",
        "        # Add chunk metadata\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata.update({\n",
        "                'chunk_id': i,\n",
        "                'chunk_size': len(chunk.page_content),\n",
        "                'processing_timestamp': str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "        return chunks\n",
        "\n",
        "# Test the document processor\n",
        "print(\"\\nüß™ Testing Document Processor...\")\n",
        "doc_processor = DocumentProcessor()\n",
        "print(\"Document processor ready!\")"
      ],
      "metadata": {
        "id": "9aIVP-nR3-0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStoreManager:\n",
        "    \"\"\"Manages document embeddings and similarity search\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "            encode_kwargs={'normalize_embeddings': True}\n",
        "        )\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, chunks: List[Document]) -> bool:\n",
        "        \"\"\"Create FAISS vector store from document chunks\"\"\"\n",
        "        try:\n",
        "            print(\"‚ö° Creating vector embeddings...\")\n",
        "            print(f\"üìä Processing {len(chunks)} chunks...\")\n",
        "\n",
        "            # Create embeddings and vector store\n",
        "            self.vector_store = FAISS.from_documents(\n",
        "                documents=chunks,\n",
        "                embedding=self.embeddings\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ Vector store created successfully!\")\n",
        "            print(f\"üìà Vector dimension: {self.vector_store.index.d}\")\n",
        "            print(f\"üìö Total vectors: {self.vector_store.index.ntotal}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error creating vector store: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def search_similar(self, query: str, k: int = 5) -> List[tuple]:\n",
        "        \"\"\"Search for similar documents\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Get documents with similarity scores\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "            print(f\"üîç Found {len(results)} relevant chunks for query\")\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Search error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get vector store statistics\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return {\"status\": \"not_created\"}\n",
        "\n",
        "        return {\n",
        "            \"status\": \"ready\",\n",
        "            \"total_vectors\": self.vector_store.index.ntotal,\n",
        "            \"vector_dimension\": self.vector_store.index.d,\n",
        "            \"model_name\": self.model_name\n",
        "        }\n",
        "\n",
        "# Test vector store manager\n",
        "print(\"\\nüß™ Testing Vector Store Manager...\")\n",
        "vector_manager = VectorStoreManager()\n",
        "print(\"Vector store manager ready!\")"
      ],
      "metadata": {
        "id": "0GDZpqx34KfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerGenerator:\n",
        "    \"\"\"Generates answers using retrieved context\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.model_name = \"microsoft/DialoGPT-medium\"\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Initialize the text generation model\"\"\"\n",
        "        try:\n",
        "            print(\"ü§ñ Loading language model...\")\n",
        "\n",
        "            # Use a simple but effective model for answer generation\n",
        "            self.generator = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=self.model_name,\n",
        "                tokenizer=self.model_name,\n",
        "                max_length=512,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=50256,\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ Language model loaded!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model: {str(e)}\")\n",
        "            # Fallback to a simpler approach\n",
        "            self.generator = None\n",
        "\n",
        "    def generate_answer(self, question: str, context: str, max_length: int = 200) -> str:\n",
        "        \"\"\"Generate answer based on context\"\"\"\n",
        "\n",
        "        if not self.generator:\n",
        "            return self._simple_answer(question, context)\n",
        "\n",
        "        try:\n",
        "            # Create a focused prompt\n",
        "            prompt = f\"\"\"\n",
        "Based on the research paper context below, answer the question concisely and accurately.\n",
        "\n",
        "Context: {context[:1000]}...\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "            # Generate response\n",
        "            response = self.generator(\n",
        "                prompt,\n",
        "                max_length=len(prompt) + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.3,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "            # Extract just the answer part\n",
        "            full_response = response[0]['generated_text']\n",
        "            answer = full_response[len(prompt):].strip()\n",
        "\n",
        "            # Clean up the answer\n",
        "            answer = self._clean_answer(answer)\n",
        "\n",
        "            return answer if answer else self._simple_answer(question, context)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Generation error: {str(e)}\")\n",
        "            return self._simple_answer(question, context)\n",
        "\n",
        "    def _simple_answer(self, question: str, context: str) -> str:\n",
        "        \"\"\"Fallback method for answer generation\"\"\"\n",
        "        # Simple extractive approach\n",
        "        sentences = context.split('.')\n",
        "        relevant_sentences = []\n",
        "\n",
        "        question_words = set(question.lower().split())\n",
        "\n",
        "        for sentence in sentences[:5]:  # Take first 5 sentences\n",
        "            sentence_words = set(sentence.lower().split())\n",
        "            if question_words.intersection(sentence_words):\n",
        "                relevant_sentences.append(sentence.strip())\n",
        "\n",
        "        if relevant_sentences:\n",
        "            return \". \".join(relevant_sentences[:2]) + \".\"\n",
        "        else:\n",
        "            return \"Based on the provided context, \" + sentences[0][:200] + \"...\"\n",
        "\n",
        "    def _clean_answer(self, answer: str) -> str:\n",
        "        \"\"\"Clean and format the generated answer\"\"\"\n",
        "        # Remove common artifacts\n",
        "        answer = answer.replace('\\n', ' ')\n",
        "        answer = ' '.join(answer.split())  # Remove extra spaces\n",
        "\n",
        "        # Ensure it ends properly\n",
        "        if answer and not answer.endswith(('.', '!', '?')):\n",
        "            answer += '.'\n",
        "\n",
        "        return answer\n",
        "\n",
        "# Test answer generator\n",
        "print(\"\\nüß™ Testing Answer Generator...\")\n",
        "answer_gen = AnswerGenerator()\n",
        "print(\"Answer generator ready!\")\n"
      ],
      "metadata": {
        "id": "mOLQ-LoV4M6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete Retrieval-Augmented Generation system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.doc_processor = DocumentProcessor()\n",
        "        self.vector_manager = VectorStoreManager()\n",
        "        self.answer_generator = AnswerGenerator()\n",
        "        self.is_ready = False\n",
        "        self.documents_info = {}\n",
        "\n",
        "    def setup(self, pdf_paths: List[str]) -> bool:\n",
        "        \"\"\"Complete system setup\"\"\"\n",
        "        print(\"üöÄ Setting up RAG system...\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load documents\n",
        "            print(\"Step 1: Loading PDFs...\")\n",
        "            documents = self.doc_processor.load_pdfs(pdf_paths)\n",
        "\n",
        "            if not documents:\n",
        "                print(\"‚ùå No documents loaded!\")\n",
        "                return False\n",
        "\n",
        "            # Step 2: Create chunks\n",
        "            print(\"\\nStep 2: Creating text chunks...\")\n",
        "            chunks = self.doc_processor.create_chunks(documents)\n",
        "\n",
        "            # Step 3: Create vector store\n",
        "            print(\"\\nStep 3: Creating vector embeddings...\")\n",
        "            if not self.vector_manager.create_vector_store(chunks):\n",
        "                return False\n",
        "\n",
        "            # Step 4: Store document information\n",
        "            self.documents_info = {\n",
        "                'total_documents': len(documents),\n",
        "                'total_chunks': len(chunks),\n",
        "                'pdf_files': [os.path.basename(path) for path in pdf_paths],\n",
        "                'setup_complete': True\n",
        "            }\n",
        "\n",
        "            self.is_ready = True\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"üéâ RAG System Setup Complete!\")\n",
        "            print(f\"üìö Loaded: {len(documents)} pages from {len(pdf_paths)} PDFs\")\n",
        "            print(f\"üî™ Created: {len(chunks)} text chunks\")\n",
        "            print(f\"‚ö° Vector store ready with {self.vector_manager.vector_store.index.ntotal} embeddings\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Setup failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def ask_question(self, question: str, num_sources: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Ask a question and get an answer with sources\"\"\"\n",
        "\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                'answer': \"‚ùå System not ready. Please upload and process documents first.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        if not question.strip():\n",
        "            return {\n",
        "                'answer': \"‚ùå Please provide a valid question.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"üîç Processing question: '{question}'\")\n",
        "\n",
        "            # Step 1: Retrieve relevant documents\n",
        "            search_results = self.vector_manager.search_similar(question, k=num_sources)\n",
        "\n",
        "            if not search_results:\n",
        "                return {\n",
        "                    'answer': \"‚ùå No relevant information found in the documents.\",\n",
        "                    'sources': [],\n",
        "                    'confidence': 0.0\n",
        "                }\n",
        "\n",
        "            # Step 2: Prepare context and source information\n",
        "            context_parts = []\n",
        "            sources = []\n",
        "            total_score = 0\n",
        "\n",
        "            for doc, score in search_results:\n",
        "                context_parts.append(doc.page_content)\n",
        "                total_score += (1 - score)  # Convert distance to similarity\n",
        "\n",
        "                source_info = {\n",
        "                    'file': doc.metadata.get('source_file', 'Unknown'),\n",
        "                    'page': doc.metadata.get('page_number', 'Unknown'),\n",
        "                    'chunk_id': doc.metadata.get('chunk_id', 'Unknown'),\n",
        "                    'similarity_score': round(1 - score, 3),  # Convert to similarity\n",
        "                    'content_preview': doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
        "                }\n",
        "                sources.append(source_info)\n",
        "\n",
        "            # Step 3: Combine context\n",
        "            combined_context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "            # Step 4: Generate answer\n",
        "            print(\"ü§ñ Generating answer...\")\n",
        "            answer = self.answer_generator.generate_answer(question, combined_context)\n",
        "\n",
        "            # Step 5: Calculate confidence score\n",
        "            confidence = min(total_score / len(search_results), 1.0)\n",
        "\n",
        "            result = {\n",
        "                'answer': answer,\n",
        "                'sources': sources,\n",
        "                'confidence': round(confidence, 3),\n",
        "                'context_length': len(combined_context),\n",
        "                'num_sources_used': len(sources)\n",
        "            }\n",
        "\n",
        "            print(f\"‚úÖ Answer generated (confidence: {result['confidence']})\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing question: {str(e)}\")\n",
        "            return {\n",
        "                'answer': f\"‚ùå Error processing your question: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "    def get_system_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current system status and statistics\"\"\"\n",
        "        base_status = {\n",
        "            'is_ready': self.is_ready,\n",
        "            'documents_info': self.documents_info,\n",
        "            'vector_store_stats': self.vector_manager.get_stats()\n",
        "        }\n",
        "\n",
        "        if self.is_ready:\n",
        "            base_status['model_info'] = {\n",
        "                'embedding_model': self.vector_manager.model_name,\n",
        "                'generation_model': self.answer_generator.model_name,\n",
        "                'gpu_available': torch.cuda.is_available()\n",
        "            }\n",
        "\n",
        "        return base_status\n",
        "\n",
        "# Initialize the complete RAG system\n",
        "print(\"\\nüß™ Initializing Complete RAG System...\")\n",
        "rag_system = RAGSystem()\n",
        "print(\"‚úÖ RAG system ready for setup!\")\n"
      ],
      "metadata": {
        "id": "s9Gkxxjf4Xlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def upload_pdf_files():\n",
        "    \"\"\"Helper function to upload PDF files in Colab\"\"\"\n",
        "    print(\"üì§ Please upload your PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_paths = []\n",
        "    for filename, data in uploaded.items():\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Save file to current directory\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(data)\n",
        "            pdf_paths.append(filename)\n",
        "            print(f\"‚úÖ Saved: {filename}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Skipped non-PDF file: {filename}\")\n",
        "\n",
        "    return pdf_paths\n",
        "\n",
        "def setup_with_sample_data():\n",
        "    \"\"\"Setup system with sample data for demonstration\"\"\"\n",
        "    print(\"üéØ Setting up with sample data...\")\n",
        "    print(\"Note: In real usage, upload your own PDF files using upload_pdf_files()\")\n",
        "\n",
        "    # Create a sample document for demonstration\n",
        "    sample_content = \"\"\"\n",
        "    This is a sample AI research paper about Natural Language Processing.\n",
        "\n",
        "    Abstract: This paper presents a novel approach to question answering systems\n",
        "    using retrieval-augmented generation. Our method combines semantic search\n",
        "    with large language models to provide accurate and contextual answers.\n",
        "\n",
        "    Introduction: Question answering has been a fundamental challenge in NLP.\n",
        "    Recent advances in transformer models have shown promising results.\n",
        "\n",
        "    Methodology: We propose a hybrid approach that uses vector embeddings\n",
        "    for document retrieval and generative models for answer synthesis.\n",
        "\n",
        "    Results: Our system achieves 85% accuracy on benchmark datasets,\n",
        "    outperforming traditional keyword-based approaches by 20%.\n",
        "\n",
        "    Conclusion: The combination of retrieval and generation provides\n",
        "    superior performance for domain-specific question answering tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create sample PDF content (simplified)\n",
        "    from langchain.schema import Document\n",
        "\n",
        "    sample_doc = Document(\n",
        "        page_content=sample_content,\n",
        "        metadata={\n",
        "            'source_file': 'sample_paper.pdf',\n",
        "            'page_number': 1,\n",
        "            'total_pages': 1\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Process sample document\n",
        "    chunks = rag_system.doc_processor.create_chunks([sample_doc])\n",
        "\n",
        "    # Create vector store\n",
        "    if rag_system.vector_manager.create_vector_store(chunks):\n",
        "        rag_system.is_ready = True\n",
        "        rag_system.documents_info = {\n",
        "            'total_documents': 1,\n",
        "            'total_chunks': len(chunks),\n",
        "            'pdf_files': ['sample_paper.pdf'],\n",
        "            'setup_complete': True\n",
        "        }\n",
        "        print(\"‚úÖ Sample system ready for testing!\")\n",
        "        return True\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "CoVbpZAR4YsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create an interactive web interface\"\"\"\n",
        "\n",
        "    def handle_file_upload(files):\n",
        "        if not files:\n",
        "            return \"‚ùå Please upload at least one PDF file.\"\n",
        "\n",
        "        try:\n",
        "            pdf_paths = []\n",
        "            for file in files:\n",
        "                if file.name.endswith('.pdf'):\n",
        "                    pdf_paths.append(file.name)\n",
        "\n",
        "            if not pdf_paths:\n",
        "                return \"‚ùå No valid PDF files found.\"\n",
        "\n",
        "            # Setup the RAG system\n",
        "            success = rag_system.setup(pdf_paths)\n",
        "\n",
        "            if success:\n",
        "                status = rag_system.get_system_status()\n",
        "                return f\"\"\"‚úÖ Setup Complete!\n",
        "\n",
        "üìä System Status:\n",
        "‚Ä¢ Documents loaded: {status['documents_info']['total_documents']}\n",
        "‚Ä¢ Text chunks created: {status['documents_info']['total_chunks']}\n",
        "‚Ä¢ Files processed: {', '.join(status['documents_info']['pdf_files'])}\n",
        "‚Ä¢ Vector embeddings: {status['vector_store_stats']['total_vectors']}\n",
        "\n",
        "üéØ Ready to answer questions!\"\"\"\n",
        "            else:\n",
        "                return \"‚ùå Setup failed. Please check your PDF files and try again.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error during setup: {str(e)}\"\n",
        "\n",
        "    def handle_question(question):\n",
        "        if not question.strip():\n",
        "            return \"‚ùå Please enter a question.\", \"\"\n",
        "\n",
        "        result = rag_system.ask_question(question)\n",
        "\n",
        "        # Format answer\n",
        "        answer_text = f\"**Answer:** {result['answer']}\\n\\n\"\n",
        "        answer_text += f\"**Confidence:** {result['confidence']}/1.0\\n\\n\"\n",
        "\n",
        "        # Format sources\n",
        "        sources_text = \"**Sources:**\\n\\n\"\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            sources_text += f\"**{i}. {source['file']}** (Page {source['page']})\\n\"\n",
        "            sources_text += f\"   ‚Ä¢ Similarity: {source['similarity_score']}\\n\"\n",
        "            sources_text += f\"   ‚Ä¢ Preview: {source['content_preview']}\\n\\n\"\n",
        "\n",
        "        if not result['sources']:\n",
        "            sources_text = \"No sources found.\"\n",
        "\n",
        "        return answer_text, sources_text\n",
        "\n",
        "    def show_system_info():\n",
        "        status = rag_system.get_system_status()\n",
        "\n",
        "        if not status['is_ready']:\n",
        "            return \"System not ready. Please upload documents first.\"\n",
        "\n",
        "        info_text = f\"\"\"**System Information:**\n",
        "\n",
        "**Status:** {'‚úÖ Ready' if status['is_ready'] else '‚ùå Not Ready'}\n",
        "\n",
        "**Documents:**\n",
        "‚Ä¢ Total documents: {status['documents_info']['total_documents']}\n",
        "‚Ä¢ Total chunks: {status['documents_info']['total_chunks']}\n",
        "‚Ä¢ Files: {', '.join(status['documents_info']['pdf_files'])}\n",
        "\n",
        "**Vector Store:**\n",
        "‚Ä¢ Total vectors: {status['vector_store_stats']['total_vectors']}\n",
        "‚Ä¢ Vector dimension: {status['vector_store_stats']['vector_dimension']}\n",
        "‚Ä¢ Embedding model: {status['vector_store_stats']['model_name']}\n",
        "\n",
        "**Models:**\n",
        "‚Ä¢ Generation model: {status.get('model_info', {}).get('generation_model', 'N/A')}\n",
        "‚Ä¢ GPU available: {status.get('model_info', {}).get('gpu_available', False)}\n",
        "\"\"\"\n",
        "        return info_text\n",
        "\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(\n",
        "        title=\"RAG System - AI Research Papers QA\",\n",
        "        theme=gr.themes.Soft()\n",
        "    ) as interface:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ü§ñ RAG System: AI Research Papers Q&A\n",
        "\n",
        "        **Upload your AI research papers and ask questions about them!**\n",
        "\n",
        "        This system uses Retrieval-Augmented Generation to:\n",
        "        - üìñ Process and understand your research papers\n",
        "        - üîç Find relevant information for your questions\n",
        "        - ü§ñ Generate accurate answers with source citations\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"üì§ Upload & Setup\"):\n",
        "            gr.Markdown(\"### Step 1: Upload Your PDF Research Papers\")\n",
        "\n",
        "            file_upload = gr.File(\n",
        "                label=\"Select PDF Files\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\"],\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            setup_btn = gr.Button(\"üöÄ Process Documents\", variant=\"primary\", size=\"lg\")\n",
        "            setup_output = gr.Textbox(\n",
        "                label=\"Setup Status\",\n",
        "                lines=10,\n",
        "                interactive=False,\n",
        "                placeholder=\"Upload PDFs and click 'Process Documents' to begin...\"\n",
        "            )\n",
        "\n",
        "            # Sample data button for demo\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### Or Try with Sample Data\")\n",
        "            sample_btn = gr.Button(\"üéØ Use Sample Data (Demo)\", variant=\"secondary\")\n",
        "\n",
        "            def setup_sample():\n",
        "                success = setup_with_sample_data()\n",
        "                if success:\n",
        "                    status = rag_system.get_system_status()\n",
        "                    return f\"\"\"‚úÖ Sample System Ready!\n",
        "\n",
        "This is a demo with sample AI research paper content.\n",
        "You can now ask questions like:\n",
        "‚Ä¢ \"What is the main contribution of this paper?\"\n",
        "‚Ä¢ \"What methodology was used?\"\n",
        "‚Ä¢ \"What were the results?\"\n",
        "\n",
        "üìä System loaded with {status['documents_info']['total_chunks']} text chunks.\"\"\"\n",
        "                else:\n",
        "                    return \"‚ùå Failed to setup sample data.\"\n",
        "\n",
        "            sample_btn.click(setup_sample, outputs=[setup_output])\n",
        "            setup_btn.click(handle_file_upload, inputs=[file_upload], outputs=[setup_output])\n",
        "\n",
        "        with gr.Tab(\"‚ùì Ask Questions\"):\n",
        "            gr.Markdown(\"### Ask Questions About Your Research Papers\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=4):\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"Your Question\",\n",
        "                        placeholder=\"e.g., What are the main contributions of this research?\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                with gr.Column(scale=1):\n",
        "                    ask_btn = gr.Button(\"üîç Get Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    answer_output = gr.Textbox(\n",
        "                        label=\"Answer\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    sources_output = gr.Textbox(\n",
        "                        label=\"Sources & Citations\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            # Sample questions\n",
        "            gr.Markdown(\"### üí° Sample Questions to Try:\")\n",
        "            sample_questions = [\n",
        "                \"What are the main contributions of this research?\",\n",
        "                \"What methodology was used in this study?\",\n",
        "                \"What are the key findings and results?\",\n",
        "                \"What are the limitations mentioned?\",\n",
        "                \"What future work is suggested?\"\n",
        "            ]\n",
        "\n",
        "            for q in sample_questions:\n",
        "                sample_q_btn = gr.Button(f\"üìù {q}\", variant=\"secondary\", size=\"sm\")\n",
        "                sample_q_btn.click(lambda x=q: x, outputs=[question_input])\n",
        "\n",
        "            ask_btn.click(\n",
        "                handle_question,\n",
        "                inputs=[question_input],\n",
        "                outputs=[answer_output, sources_output]\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"‚ÑπÔ∏è System Info\"):\n",
        "            gr.Markdown(\"### System Status and Information\")\n",
        "\n",
        "            info_btn = gr.Button(\"üîÑ Refresh System Info\", variant=\"primary\")\n",
        "            info_output = gr.Textbox(\n",
        "                label=\"System Information\",\n",
        "                lines=15,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            info_btn.click(show_system_info, outputs=[info_output])\n",
        "\n",
        "            # Auto-load info on tab open\n",
        "            interface.load(show_system_info, outputs=[info_output])\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "print(\"\\nüé® Creating Gradio Interface...\")\n",
        "interface = create_gradio_interface()\n",
        "print(\"‚úÖ Interface ready!\")"
      ],
      "metadata": {
        "id": "2mwlIx2d4pfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_system():\n",
        "    \"\"\"Launch the complete system\"\"\"\n",
        "    print(\"üöÄ Launching RAG System...\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"üìã Instructions:\")\n",
        "    print(\"1. Run this cell to launch the web interface\")\n",
        "    print(\"2. Upload your PDF research papers in the 'Upload & Setup' tab\")\n",
        "    print(\"3. Wait for processing to complete\")\n",
        "    print(\"4. Go to 'Ask Questions' tab and start asking!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Launch interface\n",
        "    interface.launch(\n",
        "        share=True,  # Creates a public shareable link\n",
        "        debug=True,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "# Quick test function\n",
        "def quick_test():\n",
        "    \"\"\"Quick test with sample data\"\"\"\n",
        "    print(\"üß™ Quick Test Mode\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Setup with sample data\n",
        "    if setup_with_sample_data():\n",
        "        print(\"\\n‚úÖ Sample system ready!\")\n",
        "\n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            \"What is this paper about?\",\n",
        "            \"What methodology was used?\",\n",
        "            \"What were the main results?\"\n",
        "        ]\n",
        "\n",
        "        for question in test_questions:\n",
        "            print(f\"\\n‚ùì Question: {question}\")\n",
        "            result = rag_system.ask_question(question, num_sources=2)\n",
        "            print(f\"‚úÖ Answer: {result['answer'][:200]}...\")\n",
        "            print(f\"üìö Sources: {len(result['sources'])} found\")\n",
        "            print(f\"üéØ Confidence: {result['confidence']}\")\n",
        "    else:\n",
        "        print(\"‚ùå Test setup failed\")"
      ],
      "metadata": {
        "id": "ttceAWcx_UWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_system()"
      ],
      "metadata": {
        "id": "mUlgm2hB5Hov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}