{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgtW8A0LCS1Bm+wdsMg4io",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnupriya-Selvraj/Agentic_AI_Workshop/blob/main/RAG_System_AI_Research_Papers_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-huggingface\n",
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install pypdf2 PyPDF2\n",
        "!pip install transformers torch\n",
        "!pip install gradio\n",
        "!pip install accelerate\n",
        "!pip install rank-bm25  # For hybrid search\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")\n"
      ],
      "metadata": {
        "id": "-Y5dODOv3x2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7b7a6a-0986-4d48-8c03-d0d5affe68ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.32.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.32.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-bm25) (2.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "✅ All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# BM25 for hybrid search\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(f\"🔥 GPU Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "uVLxq41332aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedDocumentProcessor:\n",
        "    \"\"\"Advanced document processing with improved chunking strategies\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=800, chunk_overlap=150):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "        # Multiple text splitters for different content types\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # Specialized splitter for academic content\n",
        "        self.academic_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\n",
        "                \"\\n## \",      # Section headers\n",
        "                \"\\n### \",     # Subsection headers\n",
        "                \"\\n\\n\",       # Paragraph breaks\n",
        "                \"\\n\",         # Line breaks\n",
        "                \". \",         # Sentence endings\n",
        "                \" \",          # Word boundaries\n",
        "                \"\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def load_pdfs(self, pdf_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"Load multiple PDF files with enhanced metadata\"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            try:\n",
        "                print(f\"📖 Loading: {os.path.basename(pdf_path)}\")\n",
        "                loader = PyPDFLoader(pdf_path)\n",
        "                documents = loader.load()\n",
        "\n",
        "                # Enhanced metadata extraction\n",
        "                for i, doc in enumerate(documents):\n",
        "                    # Clean and preprocess text\n",
        "                    cleaned_text = self._clean_text(doc.page_content)\n",
        "                    doc.page_content = cleaned_text\n",
        "\n",
        "                    # Extract section information\n",
        "                    section_info = self._extract_section_info(cleaned_text)\n",
        "\n",
        "                    doc.metadata.update({\n",
        "                        'source_file': os.path.basename(pdf_path),\n",
        "                        'page_number': i + 1,\n",
        "                        'total_pages': len(documents),\n",
        "                        'word_count': len(cleaned_text.split()),\n",
        "                        'char_count': len(cleaned_text),\n",
        "                        'section_type': section_info['type'],\n",
        "                        'section_title': section_info['title'],\n",
        "                        'has_figures': 'figure' in cleaned_text.lower(),\n",
        "                        'has_tables': 'table' in cleaned_text.lower(),\n",
        "                        'has_equations': bool(re.search(r'\\$.*?\\$|\\\\begin\\{equation\\}', cleaned_text))\n",
        "                    })\n",
        "\n",
        "                all_documents.extend(documents)\n",
        "                print(f\"✅ Loaded {len(documents)} pages from {os.path.basename(pdf_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading {pdf_path}: {str(e)}\")\n",
        "\n",
        "        return all_documents\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text content\"\"\"\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Fix common PDF extraction issues\n",
        "        text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # Add space between camelCase\n",
        "        text = re.sub(r'(\\.)([A-Z])', r'\\1 \\2', text)     # Space after periods\n",
        "        text = re.sub(r'([a-z])(\\d)', r'\\1 \\2', text)     # Space before numbers\n",
        "        text = re.sub(r'(\\d)([a-z])', r'\\1 \\2', text)     # Space after numbers\n",
        "\n",
        "        # Remove page headers/footers patterns\n",
        "        text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'^Page \\d+.*$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Clean up references and citations\n",
        "        text = re.sub(r'\\[\\d+(?:,\\s*\\d+)*\\]', '', text)  # Remove citation numbers\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _extract_section_info(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract section information from text\"\"\"\n",
        "        # Common academic section patterns\n",
        "        section_patterns = {\n",
        "            'abstract': r'(?i)^(?:abstract|summary)',\n",
        "            'introduction': r'(?i)^(?:1\\.?\\s*)?introduction',\n",
        "            'methodology': r'(?i)^(?:\\d+\\.?\\s*)?(?:method|methodology|approach)',\n",
        "            'results': r'(?i)^(?:\\d+\\.?\\s*)?(?:results?|findings?)',\n",
        "            'discussion': r'(?i)^(?:\\d+\\.?\\s*)?discussion',\n",
        "            'conclusion': r'(?i)^(?:\\d+\\.?\\s*)?(?:conclusion|conclusions?)',\n",
        "            'references': r'(?i)^(?:references?|bibliography)',\n",
        "            'related_work': r'(?i)^(?:\\d+\\.?\\s*)?(?:related work|literature review)',\n",
        "            'evaluation': r'(?i)^(?:\\d+\\.?\\s*)?(?:evaluation|experiments?)'\n",
        "        }\n",
        "\n",
        "        first_line = text.split('\\n')[0].strip()\n",
        "\n",
        "        for section_type, pattern in section_patterns.items():\n",
        "            if re.match(pattern, first_line):\n",
        "                return {'type': section_type, 'title': first_line}\n",
        "\n",
        "        return {'type': 'content', 'title': ''}\n",
        "\n",
        "    def create_smart_chunks(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Create intelligent chunks with improved context preservation\"\"\"\n",
        "        print(\"🔪 Creating smart text chunks...\")\n",
        "\n",
        "        all_chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            section_type = doc.metadata.get('section_type', 'content')\n",
        "\n",
        "            # Use appropriate splitter based on content type\n",
        "            if section_type in ['abstract', 'conclusion']:\n",
        "                # Keep abstracts and conclusions as single chunks if possible\n",
        "                if len(doc.page_content) <= self.chunk_size:\n",
        "                    chunk = Document(\n",
        "                        page_content=doc.page_content,\n",
        "                        metadata={**doc.metadata, 'chunk_id': len(all_chunks), 'is_complete_section': True}\n",
        "                    )\n",
        "                    all_chunks.append(chunk)\n",
        "                    continue\n",
        "\n",
        "            # Regular chunking with academic-aware splitting\n",
        "            chunks = self.academic_splitter.split_documents([doc])\n",
        "\n",
        "            # Post-process chunks for better coherence\n",
        "            processed_chunks = self._post_process_chunks(chunks, doc.metadata)\n",
        "            all_chunks.extend(processed_chunks)\n",
        "\n",
        "        # Add cross-references and relationships\n",
        "        all_chunks = self._add_chunk_relationships(all_chunks)\n",
        "\n",
        "        print(f\"✅ Created {len(all_chunks)} smart chunks\")\n",
        "        return all_chunks\n",
        "\n",
        "    def _post_process_chunks(self, chunks: List[Document], base_metadata: Dict) -> List[Document]:\n",
        "        \"\"\"Post-process chunks for better quality\"\"\"\n",
        "        processed_chunks = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            # Skip very short chunks\n",
        "            if len(chunk.page_content.strip()) < 50:\n",
        "                continue\n",
        "\n",
        "            # Ensure chunks end at sentence boundaries when possible\n",
        "            content = chunk.page_content.strip()\n",
        "            if not content.endswith(('.', '!', '?', ':')):\n",
        "                sentences = content.split('. ')\n",
        "                if len(sentences) > 1:\n",
        "                    content = '. '.join(sentences[:-1]) + '.'\n",
        "\n",
        "            # Add enhanced metadata\n",
        "            chunk.metadata.update({\n",
        "                **base_metadata,\n",
        "                'chunk_id': len(processed_chunks),\n",
        "                'chunk_index': i,\n",
        "                'chunk_size': len(content),\n",
        "                'sentence_count': len([s for s in content.split('.') if s.strip()]),\n",
        "                'processing_timestamp': str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "            chunk.page_content = content\n",
        "            processed_chunks.append(chunk)\n",
        "\n",
        "        return processed_chunks\n",
        "\n",
        "    def _add_chunk_relationships(self, chunks: List[Document]) -> List[Document]:\n",
        "        \"\"\"Add relationship information between chunks\"\"\"\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            # Add neighboring chunk information\n",
        "            chunk.metadata.update({\n",
        "                'prev_chunk_id': i - 1 if i > 0 else None,\n",
        "                'next_chunk_id': i + 1 if i < len(chunks) - 1 else None,\n",
        "                'total_chunks': len(chunks)\n",
        "            })\n",
        "\n",
        "        return chunks"
      ],
      "metadata": {
        "id": "9aIVP-nR3-0D"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedVectorStoreManager:\n",
        "    \"\"\"Enhanced vector store with hybrid search capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "            encode_kwargs={'normalize_embeddings': True}\n",
        "        )\n",
        "        self.vector_store = None\n",
        "        self.bm25 = None\n",
        "        self.chunks = []\n",
        "        self.chunk_texts = []\n",
        "\n",
        "    def create_vector_store(self, chunks: List[Document]) -> bool:\n",
        "        \"\"\"Create enhanced vector store with hybrid search support\"\"\"\n",
        "        try:\n",
        "            print(\"⚡ Creating advanced vector embeddings...\")\n",
        "            print(f\"📊 Processing {len(chunks)} chunks...\")\n",
        "\n",
        "            self.chunks = chunks\n",
        "            self.chunk_texts = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "            # Create FAISS vector store\n",
        "            self.vector_store = FAISS.from_documents(\n",
        "                documents=chunks,\n",
        "                embedding=self.embeddings\n",
        "            )\n",
        "\n",
        "            # Create BM25 index for keyword search\n",
        "            print(\"🔍 Building BM25 index for keyword search...\")\n",
        "            tokenized_chunks = [doc.lower().split() for doc in self.chunk_texts]\n",
        "            self.bm25 = BM25Okapi(tokenized_chunks)\n",
        "\n",
        "            print(f\"✅ Vector store created successfully!\")\n",
        "            print(f\"📈 Vector dimension: {self.vector_store.index.d}\")\n",
        "            print(f\"📚 Total vectors: {self.vector_store.index.ntotal}\")\n",
        "            print(f\"🔍 BM25 index ready with {len(tokenized_chunks)} documents\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creating vector store: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def hybrid_search(self, query: str, k: int = 10, alpha: float = 0.7) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"Perform hybrid search combining semantic and keyword search\"\"\"\n",
        "        if not self.vector_store or not self.bm25:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Semantic search\n",
        "            semantic_results = self.vector_store.similarity_search_with_score(query, k=k*2)\n",
        "\n",
        "            # Keyword search with BM25\n",
        "            query_tokens = query.lower().split()\n",
        "            bm25_scores = self.bm25.get_scores(query_tokens)\n",
        "\n",
        "            # Get top BM25 results\n",
        "            bm25_indices = np.argsort(bm25_scores)[::-1][:k*2]\n",
        "\n",
        "            # Combine results with weighted scoring\n",
        "            combined_results = {}\n",
        "\n",
        "            # Add semantic results\n",
        "            for doc, distance in semantic_results:\n",
        "                chunk_id = doc.metadata.get('chunk_id', 0)\n",
        "                semantic_score = 1 - distance  # Convert distance to similarity\n",
        "                combined_results[chunk_id] = {\n",
        "                    'document': doc,\n",
        "                    'semantic_score': semantic_score,\n",
        "                    'bm25_score': 0.0\n",
        "                }\n",
        "\n",
        "            # Add BM25 results\n",
        "            for idx in bm25_indices:\n",
        "                if idx < len(self.chunks):\n",
        "                    chunk_id = self.chunks[idx].metadata.get('chunk_id', idx)\n",
        "                    bm25_score = bm25_scores[idx]\n",
        "\n",
        "                    if chunk_id in combined_results:\n",
        "                        combined_results[chunk_id]['bm25_score'] = bm25_score\n",
        "                    else:\n",
        "                        combined_results[chunk_id] = {\n",
        "                            'document': self.chunks[idx],\n",
        "                            'semantic_score': 0.0,\n",
        "                            'bm25_score': bm25_score\n",
        "                        }\n",
        "\n",
        "            # Calculate combined scores and rank\n",
        "            final_results = []\n",
        "            for chunk_id, result in combined_results.items():\n",
        "                # Normalize BM25 scores\n",
        "                max_bm25 = max([r['bm25_score'] for r in combined_results.values()])\n",
        "                normalized_bm25 = result['bm25_score'] / max_bm25 if max_bm25 > 0 else 0\n",
        "\n",
        "                # Combined score\n",
        "                combined_score = alpha * result['semantic_score'] + (1 - alpha) * normalized_bm25\n",
        "\n",
        "                final_results.append((result['document'], 1 - combined_score))  # Convert back to distance\n",
        "\n",
        "            # Sort by combined score and return top k\n",
        "            final_results.sort(key=lambda x: x[1])\n",
        "\n",
        "            print(f\"🔍 Hybrid search found {len(final_results[:k])} relevant chunks\")\n",
        "            return final_results[:k]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Search error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def search_with_context(self, query: str, k: int = 5) -> List[Tuple[Document, float, List[Document]]]:\n",
        "        \"\"\"Search with additional context from neighboring chunks\"\"\"\n",
        "        base_results = self.hybrid_search(query, k)\n",
        "        enhanced_results = []\n",
        "\n",
        "        for doc, score in base_results:\n",
        "            # Get neighboring chunks for context\n",
        "            chunk_id = doc.metadata.get('chunk_id', 0)\n",
        "            context_chunks = []\n",
        "\n",
        "            # Previous chunk\n",
        "            if chunk_id > 0:\n",
        "                prev_chunk = next((c for c in self.chunks if c.metadata.get('chunk_id') == chunk_id - 1), None)\n",
        "                if prev_chunk:\n",
        "                    context_chunks.append(prev_chunk)\n",
        "\n",
        "            # Next chunk\n",
        "            next_chunk = next((c for c in self.chunks if c.metadata.get('chunk_id') == chunk_id + 1), None)\n",
        "            if next_chunk:\n",
        "                context_chunks.append(next_chunk)\n",
        "\n",
        "            enhanced_results.append((doc, score, context_chunks))\n",
        "\n",
        "        return enhanced_results"
      ],
      "metadata": {
        "id": "0GDZpqx34KfL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IntelligentAnswerGenerator:\n",
        "    \"\"\"Advanced answer generation with better context understanding\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.tokenizer = None\n",
        "        self.model_name = \"microsoft/DialoGPT-medium\"\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Initialize the text generation model with better configuration\"\"\"\n",
        "        try:\n",
        "            print(\"🤖 Loading enhanced language model...\")\n",
        "\n",
        "            # Try to use a better model if available\n",
        "            try:\n",
        "                # Use a more capable model for better answers\n",
        "                self.model_name = \"microsoft/DialoGPT-large\"  # Fallback to medium if large fails\n",
        "\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "                if self.tokenizer.pad_token is None:\n",
        "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "                self.generator = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=self.model_name,\n",
        "                    tokenizer=self.tokenizer,\n",
        "                    max_length=512,\n",
        "                    temperature=0.3,  # Lower temperature for more focused answers\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9,\n",
        "                    top_k=50,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    device=0 if torch.cuda.is_available() else -1\n",
        "                )\n",
        "\n",
        "            except Exception:\n",
        "                # Fallback to medium model\n",
        "                self.model_name = \"microsoft/DialoGPT-medium\"\n",
        "                self.generator = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=self.model_name,\n",
        "                    max_length=512,\n",
        "                    temperature=0.3,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=50256,\n",
        "                    device=0 if torch.cuda.is_available() else -1\n",
        "                )\n",
        "\n",
        "            print(f\"✅ Language model loaded: {self.model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading model: {str(e)}\")\n",
        "            self.generator = None\n",
        "\n",
        "    def generate_comprehensive_answer(self, question: str, context_results: List[Tuple], max_length: int = 300) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive answer with improved reasoning\"\"\"\n",
        "\n",
        "        if not context_results:\n",
        "            return {\n",
        "                'answer': \"I couldn't find relevant information to answer your question.\",\n",
        "                'confidence': 0.0,\n",
        "                'reasoning': \"No relevant context found\",\n",
        "                'answer_type': 'no_answer'\n",
        "            }\n",
        "\n",
        "        # Analyze question type\n",
        "        question_analysis = self._analyze_question(question)\n",
        "\n",
        "        # Prepare enhanced context\n",
        "        enhanced_context = self._prepare_enhanced_context(context_results, question_analysis)\n",
        "\n",
        "        # Generate answer based on question type\n",
        "        if question_analysis['type'] in ['definition', 'explanation']:\n",
        "            answer_result = self._generate_explanatory_answer(question, enhanced_context, max_length)\n",
        "        elif question_analysis['type'] == 'comparison':\n",
        "            answer_result = self._generate_comparative_answer(question, enhanced_context, max_length)\n",
        "        elif question_analysis['type'] == 'list':\n",
        "            answer_result = self._generate_list_answer(question, enhanced_context, max_length)\n",
        "        else:\n",
        "            answer_result = self._generate_general_answer(question, enhanced_context, max_length)\n",
        "\n",
        "        # Add metadata and confidence scoring\n",
        "        answer_result.update({\n",
        "            'question_type': question_analysis['type'],\n",
        "            'context_quality': self._assess_context_quality(enhanced_context, question),\n",
        "            'source_count': len(context_results)\n",
        "        })\n",
        "\n",
        "        return answer_result\n",
        "\n",
        "    def _analyze_question(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze question to determine type and key components\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Question type patterns\n",
        "        if any(word in question_lower for word in ['what is', 'define', 'explain', 'describe']):\n",
        "            question_type = 'definition'\n",
        "        elif any(word in question_lower for word in ['how does', 'how do', 'how is', 'how are']):\n",
        "            question_type = 'explanation'\n",
        "        elif any(word in question_lower for word in ['compare', 'difference', 'versus', 'vs']):\n",
        "            question_type = 'comparison'\n",
        "        elif any(word in question_lower for word in ['list', 'what are', 'which are', 'enumerate']):\n",
        "            question_type = 'list'\n",
        "        elif any(word in question_lower for word in ['why', 'reason', 'cause']):\n",
        "            question_type = 'reasoning'\n",
        "        else:\n",
        "            question_type = 'general'\n",
        "\n",
        "        # Extract key terms using spaCy\n",
        "        doc = nlp(question)\n",
        "        key_terms = [token.lemma_.lower() for token in doc if token.pos_ in ['NOUN', 'ADJ', 'VERB'] and not token.is_stop]\n",
        "\n",
        "        return {\n",
        "            'type': question_type,\n",
        "            'key_terms': key_terms,\n",
        "            'entities': [ent.text for ent in doc.ents],\n",
        "            'complexity': len([token for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ']])\n",
        "        }\n",
        "\n",
        "    def _prepare_enhanced_context(self, context_results: List[Tuple], question_analysis: Dict) -> str:\n",
        "        \"\"\"Prepare enhanced context with smart selection and ordering\"\"\"\n",
        "        context_parts = []\n",
        "        seen_content = set()\n",
        "\n",
        "        for i, (doc, score, neighbor_chunks) in enumerate(context_results):\n",
        "            # Main chunk\n",
        "            main_content = doc.page_content.strip()\n",
        "            if main_content not in seen_content:\n",
        "                # Add section information if available\n",
        "                section_info = \"\"\n",
        "                if doc.metadata.get('section_type') and doc.metadata.get('section_type') != 'content':\n",
        "                    section_info = f\"[From {doc.metadata['section_type'].title()} section] \"\n",
        "\n",
        "                context_parts.append(f\"{section_info}{main_content}\")\n",
        "                seen_content.add(main_content)\n",
        "\n",
        "            # Add relevant neighbor chunks for better context\n",
        "            for neighbor in neighbor_chunks:\n",
        "                neighbor_content = neighbor.page_content.strip()\n",
        "                if (neighbor_content not in seen_content and\n",
        "                    len(neighbor_content) > 100 and\n",
        "                    any(term in neighbor_content.lower() for term in question_analysis['key_terms'])):\n",
        "                    context_parts.append(f\"[Related context] {neighbor_content}\")\n",
        "                    seen_content.add(neighbor_content)\n",
        "\n",
        "        # Limit total context length\n",
        "        combined_context = \"\\n\\n\".join(context_parts)\n",
        "        if len(combined_context) > 2000:  # Limit context size\n",
        "            # Keep most relevant parts\n",
        "            truncated_parts = []\n",
        "            current_length = 0\n",
        "            for part in context_parts:\n",
        "                if current_length + len(part) <= 2000:\n",
        "                    truncated_parts.append(part)\n",
        "                    current_length += len(part)\n",
        "                else:\n",
        "                    break\n",
        "            combined_context = \"\\n\\n\".join(truncated_parts)\n",
        "\n",
        "        return combined_context\n",
        "\n",
        "    def _generate_explanatory_answer(self, question: str, context: str, max_length: int) -> Dict[str, Any]:\n",
        "        \"\"\"Generate detailed explanatory answers\"\"\"\n",
        "        if not self.generator:\n",
        "            return self._extract_answer_from_context(question, context)\n",
        "\n",
        "        prompt = f\"\"\"Based on the research paper context, provide a clear and comprehensive explanation.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide a detailed explanation that:\n",
        "1. Directly answers the question\n",
        "2. Uses information from the context\n",
        "3. Is clear and well-structured\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.generator(\n",
        "                prompt,\n",
        "                max_length=len(prompt) + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.2,  # Lower temperature for explanations\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.generator.tokenizer.pad_token_id if hasattr(self.generator, 'tokenizer') else 50256\n",
        "            )\n",
        "\n",
        "            answer = self._extract_and_clean_answer(response[0]['generated_text'], prompt)\n",
        "            confidence = self._calculate_confidence(answer, context, question)\n",
        "\n",
        "            return {\n",
        "                'answer': answer,\n",
        "                'confidence': confidence,\n",
        "                'reasoning': 'Generated explanatory answer based on context',\n",
        "                'answer_type': 'explanation'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Generation error: {str(e)}\")\n",
        "            return self._extract_answer_from_context(question, context)\n",
        "\n",
        "    def _generate_general_answer(self, question: str, context: str, max_length: int) -> Dict[str, Any]:\n",
        "        \"\"\"Generate general answers with fallback logic\"\"\"\n",
        "        if not self.generator:\n",
        "            return self._extract_answer_from_context(question, context)\n",
        "\n",
        "        prompt = f\"\"\"Answer the question based on the research paper context provided.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer concisely and accurately based on the context:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.generator(\n",
        "                prompt,\n",
        "                max_length=len(prompt) + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.3,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "            answer = self._extract_and_clean_answer(response[0]['generated_text'], prompt)\n",
        "            confidence = self._calculate_confidence(answer, context, question)\n",
        "\n",
        "            return {\n",
        "                'answer': answer,\n",
        "                'confidence': confidence,\n",
        "                'reasoning': 'Generated answer based on retrieved context',\n",
        "                'answer_type': 'general'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Generation error: {str(e)}\")\n",
        "            return self._extract_answer_from_context(question, context)\n",
        "\n",
        "    def _extract_answer_from_context(self, question: str, context: str) -> Dict[str, Any]:\n",
        "        \"\"\"Fallback method: extract answer directly from context\"\"\"\n",
        "        sentences = [s.strip() for s in context.split('.') if s.strip()]\n",
        "        question_words = set(question.lower().split())\n",
        "\n",
        "        # Score sentences by relevance\n",
        "        scored_sentences = []\n",
        "        for sentence in sentences:\n",
        "            sentence_words = set(sentence.lower().split())\n",
        "            overlap = len(question_words.intersection(sentence_words))\n",
        "            if overlap > 0:\n",
        "                score = overlap / len(question_words)\n",
        "                scored_sentences.append((sentence, score))\n",
        "\n",
        "        # Sort by relevance and combine top sentences\n",
        "        scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if scored_sentences:\n",
        "            # Take top 2-3 most relevant sentences\n",
        "            top_sentences = [s[0] for s in scored_sentences[:3]]\n",
        "            answer = '. '.join(top_sentences)\n",
        "            if not answer.endswith('.'):\n",
        "                answer += '.'\n",
        "\n",
        "            confidence = scored_sentences[0][1] if scored_sentences else 0.3\n",
        "        else:\n",
        "            # Fallback to first few sentences\n",
        "            answer = '. '.join(sentences[:2]) + '.' if sentences else \"No relevant information found.\"\n",
        "            confidence = 0.2\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'confidence': min(confidence, 0.8),  # Cap confidence for extracted answers\n",
        "            'reasoning': 'Extracted relevant sentences from context',\n",
        "            'answer_type': 'extracted'\n",
        "        }\n",
        "\n",
        "    def _extract_and_clean_answer(self, generated_text: str, prompt: str) -> str:\n",
        "        \"\"\"Extract and clean the generated answer\"\"\"\n",
        "        # Remove the prompt from the response\n",
        "        answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "        # Clean up common generation artifacts\n",
        "        answer = re.sub(r'^Answer:\\s*', '', answer)\n",
        "        answer = re.sub(r'\\n+', ' ', answer)\n",
        "        answer = ' '.join(answer.split())  # Normalize whitespace\n",
        "\n",
        "        # Ensure proper ending\n",
        "        if answer and not answer.endswith(('.', '!', '?')):\n",
        "            # Try to end at a sentence boundary\n",
        "            sentences = answer.split('.')\n",
        "            if len(sentences) > 1:\n",
        "                answer = '.'.join(sentences[:-1]) + '.'\n",
        "            else:\n",
        "                answer += '.'\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def _calculate_confidence(self, answer: str, context: str, question: str) -> float:\n",
        "        \"\"\"Calculate confidence score for the answer\"\"\"\n",
        "        if not answer or len(answer) < 20:\n",
        "            return 0.1\n",
        "\n",
        "        # Factor 1: Overlap between answer and context\n",
        "        answer_words = set(answer.lower().split())\n",
        "        context_words = set(context.lower().split())\n",
        "        overlap_ratio = len(answer_words.intersection(context_words)) / len(answer_words) if answer_words else 0\n",
        "\n",
        "        # Factor 2: Question term coverage\n",
        "        question_words = set(question.lower().split())\n",
        "        question_coverage = len(answer_words.intersection(question_words)) / len(question_words) if question_words else 0\n",
        "\n",
        "        # Factor 3: Answer length and structure\n",
        "        length_score = min(len(answer.split()) / 20, 1.0)  # Normalize to max 20 words\n",
        "\n",
        "        # Combine factors\n",
        "        confidence = (overlap_ratio * 0.4 + question_coverage * 0.4 + length_score * 0.2)\n",
        "\n",
        "        return min(confidence, 0.95)  # Cap at 95%\n",
        "\n",
        "    def _assess_context_quality(self, context: str, question: str) -> float:\n",
        "        \"\"\"Assess the quality of context for answering the question\"\"\"\n",
        "        if not context:\n",
        "            return 0.0\n",
        "\n",
        "        question_words = set(question.lower().split())\n",
        "        context_words = set(context.lower().split())\n",
        "\n",
        "        # Calculate coverage and relevance\n",
        "        coverage = len(question_words.intersection(context_words)) / len(question_words) if question_words else 0\n",
        "\n",
        "        # Context length factor\n",
        "        length_factor = min(len(context.split()) / 100, 1.0)\n",
        "\n",
        "        return min(coverage * 0.7 + length_factor * 0.3, 1.0)"
      ],
      "metadata": {
        "id": "mOLQ-LoV4M6q"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete Retrieval-Augmented Generation system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.doc_processor = DocumentProcessor()\n",
        "        self.vector_manager = VectorStoreManager()\n",
        "        self.answer_generator = AnswerGenerator()\n",
        "        self.is_ready = False\n",
        "        self.documents_info = {}\n",
        "\n",
        "    def setup(self, pdf_paths: List[str]) -> bool:\n",
        "        \"\"\"Complete system setup\"\"\"\n",
        "        print(\"🚀 Setting up RAG system...\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load documents\n",
        "            print(\"Step 1: Loading PDFs...\")\n",
        "            documents = self.doc_processor.load_pdfs(pdf_paths)\n",
        "\n",
        "            if not documents:\n",
        "                print(\"❌ No documents loaded!\")\n",
        "                return False\n",
        "\n",
        "            # Step 2: Create chunks\n",
        "            print(\"\\nStep 2: Creating text chunks...\")\n",
        "            chunks = self.doc_processor.create_chunks(documents)\n",
        "\n",
        "            # Step 3: Create vector store\n",
        "            print(\"\\nStep 3: Creating vector embeddings...\")\n",
        "            if not self.vector_manager.create_vector_store(chunks):\n",
        "                return False\n",
        "\n",
        "            # Step 4: Store document information\n",
        "            self.documents_info = {\n",
        "                'total_documents': len(documents),\n",
        "                'total_chunks': len(chunks),\n",
        "                'pdf_files': [os.path.basename(path) for path in pdf_paths],\n",
        "                'setup_complete': True\n",
        "            }\n",
        "\n",
        "            self.is_ready = True\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"🎉 RAG System Setup Complete!\")\n",
        "            print(f\"📚 Loaded: {len(documents)} pages from {len(pdf_paths)} PDFs\")\n",
        "            print(f\"🔪 Created: {len(chunks)} text chunks\")\n",
        "            print(f\"⚡ Vector store ready with {self.vector_manager.vector_store.index.ntotal} embeddings\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Setup failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def ask_question(self, question: str, num_sources: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Ask a question and get an answer with sources\"\"\"\n",
        "\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                'answer': \"❌ System not ready. Please upload and process documents first.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        if not question.strip():\n",
        "            return {\n",
        "                'answer': \"❌ Please provide a valid question.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"🔍 Processing question: '{question}'\")\n",
        "\n",
        "            # Step 1: Retrieve relevant documents\n",
        "            search_results = self.vector_manager.search_similar(question, k=num_sources)\n",
        "\n",
        "            if not search_results:\n",
        "                return {\n",
        "                    'answer': \"❌ No relevant information found in the documents.\",\n",
        "                    'sources': [],\n",
        "                    'confidence': 0.0\n",
        "                }\n",
        "\n",
        "            # Step 2: Prepare context and source information\n",
        "            context_parts = []\n",
        "            sources = []\n",
        "            total_score = 0\n",
        "\n",
        "            for doc, score in search_results:\n",
        "                context_parts.append(doc.page_content)\n",
        "                total_score += (1 - score)  # Convert distance to similarity\n",
        "\n",
        "                source_info = {\n",
        "                    'file': doc.metadata.get('source_file', 'Unknown'),\n",
        "                    'page': doc.metadata.get('page_number', 'Unknown'),\n",
        "                    'chunk_id': doc.metadata.get('chunk_id', 'Unknown'),\n",
        "                    'similarity_score': round(1 - score, 3),  # Convert to similarity\n",
        "                    'content_preview': doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
        "                }\n",
        "                sources.append(source_info)\n",
        "\n",
        "            # Step 3: Combine context\n",
        "            combined_context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "            # Step 4: Generate answer\n",
        "            print(\"🤖 Generating answer...\")\n",
        "            answer = self.answer_generator.generate_answer(question, combined_context)\n",
        "\n",
        "            # Step 5: Calculate confidence score\n",
        "            confidence = min(total_score / len(search_results), 1.0)\n",
        "\n",
        "            result = {\n",
        "                'answer': answer,\n",
        "                'sources': sources,\n",
        "                'confidence': round(confidence, 3),\n",
        "                'context_length': len(combined_context),\n",
        "                'num_sources_used': len(sources)\n",
        "            }\n",
        "\n",
        "            print(f\"✅ Answer generated (confidence: {result['confidence']})\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing question: {str(e)}\")\n",
        "            return {\n",
        "                'answer': f\"❌ Error processing your question: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "    def get_system_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current system status and statistics\"\"\"\n",
        "        base_status = {\n",
        "            'is_ready': self.is_ready,\n",
        "            'documents_info': self.documents_info,\n",
        "            'vector_store_stats': self.vector_manager.get_stats()\n",
        "        }\n",
        "\n",
        "        if self.is_ready:\n",
        "            base_status['model_info'] = {\n",
        "                'embedding_model': self.vector_manager.model_name,\n",
        "                'generation_model': self.answer_generator.model_name,\n",
        "                'gpu_available': torch.cuda.is_available()\n",
        "            }\n",
        "\n",
        "        return base_status\n",
        "\n",
        "# Initialize the complete RAG system\n",
        "print(\"\\n🧪 Initializing Complete RAG System...\")\n",
        "rag_system = RAGSystem()\n",
        "print(\"✅ RAG system ready for setup!\")"
      ],
      "metadata": {
        "id": "s9Gkxxjf4Xlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5061a22-9a27-46ed-992f-164903d72bdd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 Initializing Complete RAG System...\n",
            "🤖 Loading language model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Language model loaded!\n",
            "✅ RAG system ready for setup!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def upload_pdf_files():\n",
        "    \"\"\"Helper function to upload PDF files in Colab\"\"\"\n",
        "    print(\"📤 Please upload your PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_paths = []\n",
        "    for filename, data in uploaded.items():\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Save file to current directory\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(data)\n",
        "            pdf_paths.append(filename)\n",
        "            print(f\"✅ Saved: {filename}\")\n",
        "        else:\n",
        "            print(f\"⚠️ Skipped non-PDF file: {filename}\")\n",
        "\n",
        "    return pdf_paths\n",
        "\n",
        "def setup_with_sample_data():\n",
        "    \"\"\"Setup system with sample data for demonstration\"\"\"\n",
        "    print(\"🎯 Setting up with sample data...\")\n",
        "    print(\"Note: In real usage, upload your own PDF files using upload_pdf_files()\")\n",
        "\n",
        "    # Create a sample document for demonstration\n",
        "    sample_content = \"\"\"\n",
        "    This is a sample AI research paper about Natural Language Processing.\n",
        "\n",
        "    Abstract: This paper presents a novel approach to question answering systems\n",
        "    using retrieval-augmented generation. Our method combines semantic search\n",
        "    with large language models to provide accurate and contextual answers.\n",
        "\n",
        "    Introduction: Question answering has been a fundamental challenge in NLP.\n",
        "    Recent advances in transformer models have shown promising results.\n",
        "\n",
        "    Methodology: We propose a hybrid approach that uses vector embeddings\n",
        "    for document retrieval and generative models for answer synthesis.\n",
        "\n",
        "    Results: Our system achieves 85% accuracy on benchmark datasets,\n",
        "    outperforming traditional keyword-based approaches by 20%.\n",
        "\n",
        "    Conclusion: The combination of retrieval and generation provides\n",
        "    superior performance for domain-specific question answering tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create sample PDF content (simplified)\n",
        "    from langchain.schema import Document\n",
        "\n",
        "    sample_doc = Document(\n",
        "        page_content=sample_content,\n",
        "        metadata={\n",
        "            'source_file': 'sample_paper.pdf',\n",
        "            'page_number': 1,\n",
        "            'total_pages': 1\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Process sample document\n",
        "    chunks = rag_system.doc_processor.create_chunks([sample_doc])\n",
        "\n",
        "    # Create vector store\n",
        "    if rag_system.vector_manager.create_vector_store(chunks):\n",
        "        rag_system.is_ready = True\n",
        "        rag_system.documents_info = {\n",
        "            'total_documents': 1,\n",
        "            'total_chunks': len(chunks),\n",
        "            'pdf_files': ['sample_paper.pdf'],\n",
        "            'setup_complete': True\n",
        "        }\n",
        "        print(\"✅ Sample system ready for testing!\")\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n"
      ],
      "metadata": {
        "id": "CoVbpZAR4YsQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create an interactive web interface\"\"\"\n",
        "\n",
        "    def handle_file_upload(files):\n",
        "        if not files:\n",
        "            return \"❌ Please upload at least one PDF file.\"\n",
        "\n",
        "        try:\n",
        "            pdf_paths = []\n",
        "            for file in files:\n",
        "                if file.name.endswith('.pdf'):\n",
        "                    pdf_paths.append(file.name)\n",
        "\n",
        "            if not pdf_paths:\n",
        "                return \"❌ No valid PDF files found.\"\n",
        "\n",
        "            # Setup the RAG system\n",
        "            success = rag_system.setup(pdf_paths)\n",
        "\n",
        "            if success:\n",
        "                status = rag_system.get_system_status()\n",
        "                return f\"\"\"✅ Setup Complete!\n",
        "\n",
        "📊 System Status:\n",
        "• Documents loaded: {status['documents_info']['total_documents']}\n",
        "• Text chunks created: {status['documents_info']['total_chunks']}\n",
        "• Files processed: {', '.join(status['documents_info']['pdf_files'])}\n",
        "• Vector embeddings: {status['vector_store_stats']['total_vectors']}\n",
        "\n",
        "🎯 Ready to answer questions!\"\"\"\n",
        "            else:\n",
        "                return \"❌ Setup failed. Please check your PDF files and try again.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error during setup: {str(e)}\"\n",
        "\n",
        "    def handle_question(question):\n",
        "        if not question.strip():\n",
        "            return \"❌ Please enter a question.\", \"\"\n",
        "\n",
        "        result = rag_system.ask_question(question)\n",
        "\n",
        "        # Format answer\n",
        "        answer_text = f\"**Answer:** {result['answer']}\\n\\n\"\n",
        "        answer_text += f\"**Confidence:** {result['confidence']}/1.0\\n\\n\"\n",
        "\n",
        "        # Format sources\n",
        "        sources_text = \"**Sources:**\\n\\n\"\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            sources_text += f\"**{i}. {source['file']}** (Page {source['page']})\\n\"\n",
        "            sources_text += f\"   • Similarity: {source['similarity_score']}\\n\"\n",
        "            sources_text += f\"   • Preview: {source['content_preview']}\\n\\n\"\n",
        "\n",
        "        if not result['sources']:\n",
        "            sources_text = \"No sources found.\"\n",
        "\n",
        "        return answer_text, sources_text\n",
        "\n",
        "    def show_system_info():\n",
        "        status = rag_system.get_system_status()\n",
        "\n",
        "        if not status['is_ready']:\n",
        "            return \"System not ready. Please upload documents first.\"\n",
        "\n",
        "        info_text = f\"\"\"**System Information:**\n",
        "\n",
        "**Status:** {'✅ Ready' if status['is_ready'] else '❌ Not Ready'}\n",
        "\n",
        "**Documents:**\n",
        "• Total documents: {status['documents_info']['total_documents']}\n",
        "• Total chunks: {status['documents_info']['total_chunks']}\n",
        "• Files: {', '.join(status['documents_info']['pdf_files'])}\n",
        "\n",
        "**Vector Store:**\n",
        "• Total vectors: {status['vector_store_stats']['total_vectors']}\n",
        "• Vector dimension: {status['vector_store_stats']['vector_dimension']}\n",
        "• Embedding model: {status['vector_store_stats']['model_name']}\n",
        "\n",
        "**Models:**\n",
        "• Generation model: {status.get('model_info', {}).get('generation_model', 'N/A')}\n",
        "• GPU available: {status.get('model_info', {}).get('gpu_available', False)}\n",
        "\"\"\"\n",
        "        return info_text\n",
        "\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(\n",
        "        title=\"RAG System - AI Research Papers QA\",\n",
        "        theme=gr.themes.Soft()\n",
        "    ) as interface:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        # 🤖 RAG System: AI Research Papers Q&A\n",
        "\n",
        "        **Upload your AI research papers and ask questions about them!**\n",
        "\n",
        "        This system uses Retrieval-Augmented Generation to:\n",
        "        - 📖 Process and understand your research papers\n",
        "        - 🔍 Find relevant information for your questions\n",
        "        - 🤖 Generate accurate answers with source citations\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"📤 Upload & Setup\"):\n",
        "            gr.Markdown(\"### Step 1: Upload Your PDF Research Papers\")\n",
        "\n",
        "            file_upload = gr.File(\n",
        "                label=\"Select PDF Files\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\"],\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            setup_btn = gr.Button(\"🚀 Process Documents\", variant=\"primary\", size=\"lg\")\n",
        "            setup_output = gr.Textbox(\n",
        "                label=\"Setup Status\",\n",
        "                lines=10,\n",
        "                interactive=False,\n",
        "                placeholder=\"Upload PDFs and click 'Process Documents' to begin...\"\n",
        "            )\n",
        "\n",
        "            # Sample data button for demo\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### Or Try with Sample Data\")\n",
        "            sample_btn = gr.Button(\"🎯 Use Sample Data (Demo)\", variant=\"secondary\")\n",
        "\n",
        "            def setup_sample():\n",
        "                success = setup_with_sample_data()\n",
        "                if success:\n",
        "                    status = rag_system.get_system_status()\n",
        "                    return f\"\"\"✅ Sample System Ready!\n",
        "\n",
        "This is a demo with sample AI research paper content.\n",
        "You can now ask questions like:\n",
        "• \"What is the main contribution of this paper?\"\n",
        "• \"What methodology was used?\"\n",
        "• \"What were the results?\"\n",
        "\n",
        "📊 System loaded with {status['documents_info']['total_chunks']} text chunks.\"\"\"\n",
        "                else:\n",
        "                    return \"❌ Failed to setup sample data.\"\n",
        "\n",
        "            sample_btn.click(setup_sample, outputs=[setup_output])\n",
        "            setup_btn.click(handle_file_upload, inputs=[file_upload], outputs=[setup_output])\n",
        "\n",
        "        with gr.Tab(\"❓ Ask Questions\"):\n",
        "            gr.Markdown(\"### Ask Questions About Your Research Papers\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=4):\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"Your Question\",\n",
        "                        placeholder=\"e.g., What are the main contributions of this research?\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                with gr.Column(scale=1):\n",
        "                    ask_btn = gr.Button(\"🔍 Get Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    answer_output = gr.Textbox(\n",
        "                        label=\"Answer\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    sources_output = gr.Textbox(\n",
        "                        label=\"Sources & Citations\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            # Sample questions\n",
        "            gr.Markdown(\"### 💡 Sample Questions to Try:\")\n",
        "            sample_questions = [\n",
        "                \"What are the main contributions of this research?\",\n",
        "                \"What methodology was used in this study?\",\n",
        "                \"What are the key findings and results?\",\n",
        "                \"What are the limitations mentioned?\",\n",
        "                \"What future work is suggested?\"\n",
        "            ]\n",
        "\n",
        "            for q in sample_questions:\n",
        "                sample_q_btn = gr.Button(f\"📝 {q}\", variant=\"secondary\", size=\"sm\")\n",
        "                sample_q_btn.click(lambda x=q: x, outputs=[question_input])\n",
        "\n",
        "            ask_btn.click(\n",
        "                handle_question,\n",
        "                inputs=[question_input],\n",
        "                outputs=[answer_output, sources_output]\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"ℹ️ System Info\"):\n",
        "            gr.Markdown(\"### System Status and Information\")\n",
        "\n",
        "            info_btn = gr.Button(\"🔄 Refresh System Info\", variant=\"primary\")\n",
        "            info_output = gr.Textbox(\n",
        "                label=\"System Information\",\n",
        "                lines=15,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            info_btn.click(show_system_info, outputs=[info_output])\n",
        "\n",
        "            # Auto-load info on tab open\n",
        "            interface.load(show_system_info, outputs=[info_output])\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "print(\"\\n🎨 Creating Gradio Interface...\")\n",
        "interface = create_gradio_interface()\n",
        "print(\"✅ Interface ready!\")"
      ],
      "metadata": {
        "id": "2mwlIx2d4pfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee979f4e-8ded-4e19-d333-844e93fdb6b8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎨 Creating Gradio Interface...\n",
            "✅ Interface ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf PyPDF\n",
        "\n",
        "def run_system():\n",
        "    \"\"\"Launch the complete system\"\"\"\n",
        "    print(\"🚀 Launching RAG System...\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"📋 Instructions:\")\n",
        "    print(\"1. Run this cell to launch the web interface\")\n",
        "    print(\"2. Upload your PDF research papers in the 'Upload & Setup' tab\")\n",
        "    print(\"3. Wait for processing to complete\")\n",
        "    print(\"4. Go to 'Ask Questions' tab and start asking!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Launch interface\n",
        "    interface.launch(\n",
        "        share=True,  # Creates a public shareable link\n",
        "        debug=True,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "# Quick test function\n",
        "def quick_test():\n",
        "    \"\"\"Quick test with sample data\"\"\"\n",
        "    print(\"🧪 Quick Test Mode\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Setup with sample data\n",
        "    if setup_with_sample_data():\n",
        "        print(\"\\n✅ Sample system ready!\")\n",
        "\n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            \"What is this paper about?\",\n",
        "            \"What methodology was used?\",\n",
        "            \"What were the main results?\"\n",
        "        ]\n",
        "\n",
        "        for question in test_questions:\n",
        "            print(f\"\\n❓ Question: {question}\")\n",
        "            result = rag_system.ask_question(question, num_sources=2)\n",
        "            print(f\"✅ Answer: {result['answer'][:200]}...\")\n",
        "            print(f\"📚 Sources: {len(result['sources'])} found\")\n",
        "            print(f\"🎯 Confidence: {result['confidence']}\")\n",
        "    else:\n",
        "        print(\"❌ Test setup failed\")"
      ],
      "metadata": {
        "id": "ttceAWcx_UWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abdf2707-ece9-44ab-e5dc-ce8a81ec864d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_system()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mUlgm2hB5Hov",
        "outputId": "7c7039a1-31ca-43d1-a35e-8bf0c6133891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Launching RAG System...\n",
            "============================================================\n",
            "📋 Instructions:\n",
            "1. Run this cell to launch the web interface\n",
            "2. Upload your PDF research papers in the 'Upload & Setup' tab\n",
            "3. Wait for processing to complete\n",
            "4. Go to 'Ask Questions' tab and start asking!\n",
            "============================================================\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3897e081044a71abde.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3897e081044a71abde.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Setting up RAG system...\n",
            "==================================================\n",
            "Step 1: Loading PDFs...\n",
            "📖 Loading: 1706.03762v7.pdf\n",
            "✅ Loaded 15 pages from 1706.03762v7.pdf\n",
            "📖 Loading: 2005.11401v4.pdf\n",
            "✅ Loaded 19 pages from 2005.11401v4.pdf\n",
            "📖 Loading: 2005.14165v4.pdf\n",
            "✅ Loaded 75 pages from 2005.14165v4.pdf\n",
            "\n",
            "Step 2: Creating text chunks...\n",
            "🔪 Creating text chunks...\n",
            "✅ Created 461 chunks\n",
            "\n",
            "Step 3: Creating vector embeddings...\n",
            "⚡ Creating vector embeddings...\n",
            "📊 Processing 461 chunks...\n",
            "✅ Vector store created successfully!\n",
            "📈 Vector dimension: 384\n",
            "📚 Total vectors: 461\n",
            "\n",
            "==================================================\n",
            "🎉 RAG System Setup Complete!\n",
            "📚 Loaded: 109 pages from 3 PDFs\n",
            "🔪 Created: 461 text chunks\n",
            "⚡ Vector store ready with 461 embeddings\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing question: 'What are the main components of a RAG model, and how do they interact? '\n",
            "🔍 Found 3 relevant chunks for query\n",
            "🤖 Generating answer...\n",
            "✅ Answer generated (confidence: 0.14800000190734863)\n",
            "🔍 Processing question: 'What are the two sub-layers in each encoder layer of the Transformer model?'\n",
            "🔍 Found 3 relevant chunks for query\n",
            "🤖 Generating answer...\n",
            "✅ Answer generated (confidence: 0.16200000047683716)\n",
            "🔍 Processing question: 'Explain how positional encoding is implemented in Transformers and why it is necessary. Describe the concept of multi-head attention in the Transformer architecture. Why is it beneficial?'\n",
            "🔍 Found 3 relevant chunks for query\n",
            "🤖 Generating answer...\n",
            "✅ Answer generated (confidence: 0.2750000059604645)\n"
          ]
        }
      ]
    }
  ]
}