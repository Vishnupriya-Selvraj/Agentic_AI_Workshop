{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMS/4uZ4eDgsB8rj5bXqN87",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnupriya-Selvraj/Agentic_AI_Workshop/blob/main/Day-3/RAG%20System%20AI%20Research%20Papers%20Question%20Answering/RAG_System_AI_Research_Papers_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 1: Installation and Setup**\n",
        "\n",
        "**Installs core dependencies:**"
      ],
      "metadata": {
        "id": "1rpCV7XcMtb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-huggingface\n",
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install pypdf2 PyPDF2\n",
        "!pip install transformers torch\n",
        "!pip install gradio\n",
        "!pip install accelerate\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")"
      ],
      "metadata": {
        "id": "-Y5dODOv3x2e",
        "outputId": "e76e0618-bc93-4aef-9163-41dd6ef87641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.34.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n",
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Packages:**\n",
        "\n",
        "*  langchain - Framework for building RAG\n",
        "systems\n",
        "\n",
        "*   sentence-transformers - For generating text embeddings\n",
        "\n",
        "*   faiss-cpu - Efficient vector similarity search\n",
        "\n",
        "*   PyPDF2 - PDF text extraction\n",
        "\n",
        "*   transformers - Language models for answer generation\n",
        "\n",
        "*   gradio - Web interface builder"
      ],
      "metadata": {
        "id": "YE1elSOrM34x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 2: Import Libraries**\n",
        "**Loads all required modules:**"
      ],
      "metadata": {
        "id": "StTN9anGNZoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from typing import List, Dict, Any\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(f\"🔥 GPU Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "uVLxq41332aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Critical Components:**\n",
        "\n",
        "*   PDF processing tools (PyPDFLoader)\n",
        "\n",
        "*   Embedding models (HuggingFaceEmbeddings)\n",
        "\n",
        "*   Vector database (FAISS)\n",
        "\n",
        "*   GPU check (torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "9esdukj_Niv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 3: Document Processor**\n",
        "**Handles PDF loading and preprocessing:**"
      ],
      "metadata": {
        "id": "jOoAoYO_Nrkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"Handles loading and processing of PDF documents with better chunking\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=100):  # Smaller chunks for better precision\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Better separators\n",
        "        )\n",
        "\n",
        "    def load_pdfs(self, pdf_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"Load multiple PDF files and return documents with better preprocessing\"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            try:\n",
        "                print(f\"📖 Loading: {os.path.basename(pdf_path)}\")\n",
        "                loader = PyPDFLoader(pdf_path)\n",
        "                documents = loader.load()\n",
        "\n",
        "                # Clean and preprocess documents\n",
        "                for i, doc in enumerate(documents):\n",
        "                    # Clean the text content\n",
        "                    cleaned_content = self._clean_text(doc.page_content)\n",
        "                    doc.page_content = cleaned_content\n",
        "\n",
        "                    # Add comprehensive metadata\n",
        "                    doc.metadata.update({\n",
        "                        'source_file': os.path.basename(pdf_path),\n",
        "                        'page_number': i + 1,\n",
        "                        'total_pages': len(documents),\n",
        "                        'char_count': len(cleaned_content)\n",
        "                    })\n",
        "\n",
        "                all_documents.extend(documents)\n",
        "                print(f\"✅ Loaded {len(documents)} pages from {os.path.basename(pdf_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading {pdf_path}: {str(e)}\")\n",
        "\n",
        "        return all_documents\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text content\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove page numbers and headers/footers (simple heuristic)\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            # Skip very short lines that might be headers/footers\n",
        "            if len(line) > 10 and not re.match(r'^\\d+$', line):\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        return ' '.join(cleaned_lines)\n",
        "\n",
        "    def create_chunks(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks with better metadata\"\"\"\n",
        "        print(\"🔪 Creating text chunks...\")\n",
        "\n",
        "        chunks = self.text_splitter.split_documents(documents)\n",
        "\n",
        "        # Add enhanced chunk metadata\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata.update({\n",
        "                'chunk_id': i,\n",
        "                'chunk_size': len(chunk.page_content),\n",
        "                'word_count': len(chunk.page_content.split()),\n",
        "                'processing_timestamp': str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "        print(f\"✅ Created {len(chunks)} chunks\")\n",
        "        return chunks\n"
      ],
      "metadata": {
        "id": "9aIVP-nR3-0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Features:**\n",
        "\n",
        "*   Chunk size customization (default: 1000 chars)\n",
        "\n",
        "*   Metadata preservation (source file, page numbers)\n",
        "\n",
        "*   Smart text splitting at natural boundaries"
      ],
      "metadata": {
        "id": "213IVi8GNyum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 4: Vector Store Manager**\n",
        "**Manages document embeddings:**"
      ],
      "metadata": {
        "id": "KYJUE3fQN1zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStoreManager:\n",
        "    \"\"\"Manages document embeddings with better similarity search\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "            encode_kwargs={\n",
        "                'normalize_embeddings': True,\n",
        "                'batch_size': 16  # Better batch size for processing\n",
        "            }\n",
        "        )\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, chunks: List[Document]) -> bool:\n",
        "        \"\"\"Create FAISS vector store with better configuration\"\"\"\n",
        "        try:\n",
        "            print(\"⚡ Creating vector embeddings...\")\n",
        "            print(f\"📊 Processing {len(chunks)} chunks...\")\n",
        "\n",
        "            # Filter out very short chunks that might not be meaningful\n",
        "            filtered_chunks = [chunk for chunk in chunks if len(chunk.page_content.strip()) > 20]\n",
        "\n",
        "            print(f\"📊 Using {len(filtered_chunks)} meaningful chunks...\")\n",
        "\n",
        "            # Create embeddings and vector store\n",
        "            self.vector_store = FAISS.from_documents(\n",
        "                documents=filtered_chunks,\n",
        "                embedding=self.embeddings\n",
        "            )\n",
        "\n",
        "            print(f\"✅ Vector store created successfully!\")\n",
        "            print(f\"📈 Vector dimension: {self.vector_store.index.d}\")\n",
        "            print(f\"📚 Total vectors: {self.vector_store.index.ntotal}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creating vector store: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def search_similar(self, query: str, k: int = 5) -> List[tuple]:\n",
        "        \"\"\"Search for similar documents with better scoring\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Use MMR (Maximum Marginal Relevance) for better diversity\n",
        "            try:\n",
        "                results = self.vector_store.max_marginal_relevance_search_with_score(\n",
        "                    query, k=k, fetch_k=k*2\n",
        "                )\n",
        "            except:\n",
        "                # Fallback to regular similarity search\n",
        "                results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "            print(f\"🔍 Found {len(results)} relevant chunks for query\")\n",
        "\n",
        "            # Filter results by minimum similarity threshold\n",
        "            filtered_results = []\n",
        "            for doc, score in results:\n",
        "                # Convert distance to similarity and filter low scores\n",
        "                similarity = 1 - score\n",
        "                if similarity > 0.1:  # Minimum threshold\n",
        "                    filtered_results.append((doc, score))\n",
        "\n",
        "            return filtered_results[:k]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Search error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get vector store statistics\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return {\"status\": \"not_created\"}\n",
        "\n",
        "        return {\n",
        "            \"status\": \"ready\",\n",
        "            \"total_vectors\": self.vector_store.index.ntotal,\n",
        "            \"vector_dimension\": self.vector_store.index.d,\n",
        "            \"model_name\": self.model_name\n",
        "        }"
      ],
      "metadata": {
        "id": "0GDZpqx34KfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Technical Details:**\n",
        "\n",
        "*   Uses **`all-MiniLM-L6-v2`** embedding model\n",
        "\n",
        "*   Automatic GPU/CPU switching\n",
        "\n",
        "*   Returns similarity scores with results"
      ],
      "metadata": {
        "id": "G5Di-qs6N5u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 5: Answer Generator**\n",
        "**Produces human-readable answers:**"
      ],
      "metadata": {
        "id": "oXoEw8-ON_76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerGenerator:\n",
        "    \"\"\"Generates answers using retrieved context with better models and prompting\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.model_name = \"microsoft/DialoGPT-medium\"  # Keep as fallback\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Initialize the text generation model with better configuration\"\"\"\n",
        "        try:\n",
        "            print(\"🤖 Loading language model...\")\n",
        "\n",
        "            # Try to use a better model for QA if available\n",
        "            try:\n",
        "                # Use a more suitable model for question answering\n",
        "                from transformers import pipeline\n",
        "                self.generator = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=\"microsoft/DialoGPT-medium\",\n",
        "                    tokenizer=\"microsoft/DialoGPT-medium\",\n",
        "                    max_new_tokens=150,  # Use max_new_tokens instead of max_length\n",
        "                    temperature=0.3,  # Lower temperature for more focused answers\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=50256,\n",
        "                    device=0 if torch.cuda.is_available() else -1,\n",
        "                    truncation=True  # Explicitly enable truncation\n",
        "                )\n",
        "                print(\"✅ Language model loaded!\")\n",
        "\n",
        "            except Exception as model_error:\n",
        "                print(f\"⚠️ Could not load advanced model: {model_error}\")\n",
        "                self.generator = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading model: {str(e)}\")\n",
        "            self.generator = None\n",
        "\n",
        "    def generate_answer(self, question: str, context: str, max_length: int = 200) -> str:\n",
        "        \"\"\"Generate answer based on context with improved prompting\"\"\"\n",
        "\n",
        "        # Use extractive approach as primary method for better accuracy\n",
        "        extractive_answer = self._extractive_answer(question, context)\n",
        "\n",
        "        if self.generator:\n",
        "            try:\n",
        "                # Create a better prompt for QA\n",
        "                prompt = f\"\"\"Context: {context[:800]}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Based on the context above, provide a clear and accurate answer. If the answer is not in the context, say so.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "                # Generate response with better parameters\n",
        "                response = self.generator(\n",
        "                    prompt,\n",
        "                    max_new_tokens=100,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.1,  # Very low temperature for factual answers\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=50256,\n",
        "                    truncation=True\n",
        "                )\n",
        "\n",
        "                # Extract answer\n",
        "                full_response = response[0]['generated_text']\n",
        "                if \"Answer:\" in full_response:\n",
        "                    answer = full_response.split(\"Answer:\")[-1].strip()\n",
        "                    answer = self._clean_answer(answer)\n",
        "\n",
        "                    # If generated answer is too short or generic, use extractive\n",
        "                    if len(answer) < 20 or \"based on\" in answer.lower()[:20]:\n",
        "                        return extractive_answer\n",
        "\n",
        "                    return answer\n",
        "                else:\n",
        "                    return extractive_answer\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Generation error: {str(e)}\")\n",
        "                return extractive_answer\n",
        "\n",
        "        return extractive_answer\n",
        "\n",
        "    def _extractive_answer(self, question: str, context: str) -> str:\n",
        "        \"\"\"Improved extractive answer generation\"\"\"\n",
        "        # Split context into sentences\n",
        "        sentences = [s.strip() for s in context.replace('\\n', ' ').split('.') if s.strip()]\n",
        "\n",
        "        # Get question keywords\n",
        "        question_lower = question.lower()\n",
        "        question_words = set(question_lower.split())\n",
        "\n",
        "        # Remove common question words\n",
        "        stop_words = {'what', 'is', 'are', 'how', 'why', 'when', 'where', 'who', 'which', 'the', 'a', 'an'}\n",
        "        key_words = question_words - stop_words\n",
        "\n",
        "        # Score sentences based on keyword overlap and position\n",
        "        scored_sentences = []\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if len(sentence) < 10:  # Skip very short sentences\n",
        "                continue\n",
        "\n",
        "            sentence_lower = sentence.lower()\n",
        "            sentence_words = set(sentence_lower.split())\n",
        "\n",
        "            # Calculate relevance score\n",
        "            overlap = len(key_words.intersection(sentence_words))\n",
        "            position_score = 1.0 / (i + 1)  # Earlier sentences get higher scores\n",
        "            length_score = min(len(sentence) / 100, 1.0)  # Prefer moderately long sentences\n",
        "\n",
        "            total_score = overlap * 2 + position_score + length_score\n",
        "\n",
        "            if overlap > 0:  # Only consider sentences with keyword overlap\n",
        "                scored_sentences.append((total_score, sentence, i))\n",
        "\n",
        "        # Sort by score and take top sentences\n",
        "        scored_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "        if not scored_sentences:\n",
        "            # Fallback: return first few sentences\n",
        "            return \". \".join(sentences[:2]) + \".\" if sentences else \"No relevant information found in the context.\"\n",
        "\n",
        "        # Combine top 2-3 relevant sentences\n",
        "        top_sentences = [sent[1] for sent in scored_sentences[:2]]\n",
        "        answer = \". \".join(top_sentences)\n",
        "\n",
        "        # Clean and format\n",
        "        answer = self._clean_answer(answer)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def _clean_answer(self, answer: str) -> str:\n",
        "        \"\"\"Clean and format the generated answer\"\"\"\n",
        "        if not answer:\n",
        "            return \"No answer could be generated from the provided context.\"\n",
        "\n",
        "        # Remove extra whitespace and normalize\n",
        "        answer = ' '.join(answer.split())\n",
        "\n",
        "        # Remove common artifacts\n",
        "        answer = answer.replace('\\\\n', ' ')\n",
        "\n",
        "        # Ensure proper ending\n",
        "        if answer and not answer.endswith(('.', '!', '?')):\n",
        "            answer += '.'\n",
        "\n",
        "        # Limit length\n",
        "        if len(answer) > 300:\n",
        "            # Try to cut at a sentence boundary\n",
        "            sentences = answer.split('.')\n",
        "            if len(sentences) > 1:\n",
        "                answer = '. '.join(sentences[:-1]) + '.'\n",
        "            else:\n",
        "                answer = answer[:297] + '...'\n",
        "\n",
        "        return answer\n"
      ],
      "metadata": {
        "id": "mOLQ-LoV4M6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generation Pipeline:**\n",
        "\n",
        "*   Creates LLM prompt with question + context\n",
        "\n",
        "*   Uses DialoGPT-medium for generation\n",
        "\n",
        "*   Fallback to extractive QA if generation fails"
      ],
      "metadata": {
        "id": "ry0MyXxSOF1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 6: RAG System**\n",
        "**Main orchestrator class:**"
      ],
      "metadata": {
        "id": "F9KjDwa5OJYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete Retrieval-Augmented Generation system with improved accuracy\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.doc_processor = DocumentProcessor(chunk_size=500, chunk_overlap=100)  # Better chunk size\n",
        "        self.vector_manager = VectorStoreManager()\n",
        "        self.answer_generator = AnswerGenerator()\n",
        "        self.is_ready = False\n",
        "        self.documents_info = {}\n",
        "\n",
        "    def setup(self, pdf_paths: List[str]) -> bool:\n",
        "        \"\"\"Complete system setup with better error handling\"\"\"\n",
        "        print(\"🚀 Setting up RAG system...\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load documents\n",
        "            print(\"Step 1: Loading PDFs...\")\n",
        "            documents = self.doc_processor.load_pdfs(pdf_paths)\n",
        "\n",
        "            if not documents:\n",
        "                print(\"❌ No documents loaded!\")\n",
        "                return False\n",
        "\n",
        "            # Step 2: Create chunks\n",
        "            print(\"\\nStep 2: Creating text chunks...\")\n",
        "            chunks = self.doc_processor.create_chunks(documents)\n",
        "\n",
        "            if not chunks:\n",
        "                print(\"❌ No chunks created!\")\n",
        "                return False\n",
        "\n",
        "            # Step 3: Create vector store\n",
        "            print(\"\\nStep 3: Creating vector embeddings...\")\n",
        "            if not self.vector_manager.create_vector_store(chunks):\n",
        "                return False\n",
        "\n",
        "            # Step 4: Store document information\n",
        "            self.documents_info = {\n",
        "                'total_documents': len(documents),\n",
        "                'total_chunks': len(chunks),\n",
        "                'pdf_files': [os.path.basename(path) for path in pdf_paths],\n",
        "                'setup_complete': True\n",
        "            }\n",
        "\n",
        "            self.is_ready = True\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"🎉 RAG System Setup Complete!\")\n",
        "            print(f\"📚 Loaded: {len(documents)} pages from {len(pdf_paths)} PDFs\")\n",
        "            print(f\"🔪 Created: {len(chunks)} text chunks\")\n",
        "            print(f\"⚡ Vector store ready with {self.vector_manager.vector_store.index.ntotal} embeddings\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Setup failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def ask_question(self, question: str, num_sources: int = 4) -> Dict[str, Any]:\n",
        "        \"\"\"Ask a question with improved processing and answer generation\"\"\"\n",
        "\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                'answer': \"❌ System not ready. Please upload and process documents first.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        if not question.strip():\n",
        "            return {\n",
        "                'answer': \"❌ Please provide a valid question.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"🔍 Processing question: '{question}'\")\n",
        "\n",
        "            # Enhance the question for better retrieval\n",
        "            enhanced_query = self._enhance_query(question)\n",
        "            print(f\"🔍 Enhanced query: '{enhanced_query}'\")\n",
        "\n",
        "            # Step 1: Retrieve relevant documents\n",
        "            search_results = self.vector_manager.search_similar(enhanced_query, k=num_sources)\n",
        "\n",
        "            if not search_results:\n",
        "                return {\n",
        "                    'answer': \"❌ No relevant information found in the documents for this question.\",\n",
        "                    'sources': [],\n",
        "                    'confidence': 0.0\n",
        "                }\n",
        "\n",
        "            # Step 2: Prepare context and source information\n",
        "            context_parts = []\n",
        "            sources = []\n",
        "            similarity_scores = []\n",
        "\n",
        "            for doc, score in search_results:\n",
        "                similarity = 1 - score  # Convert distance to similarity\n",
        "                similarity_scores.append(similarity)\n",
        "\n",
        "                context_parts.append(doc.page_content)\n",
        "\n",
        "                source_info = {\n",
        "                    'file': doc.metadata.get('source_file', 'Unknown'),\n",
        "                    'page': doc.metadata.get('page_number', 'Unknown'),\n",
        "                    'chunk_id': doc.metadata.get('chunk_id', 'Unknown'),\n",
        "                    'similarity_score': round(similarity, 3),\n",
        "                    'content_preview': doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
        "                }\n",
        "                sources.append(source_info)\n",
        "\n",
        "            # Step 3: Combine context intelligently\n",
        "            combined_context = self._combine_context(context_parts, question)\n",
        "\n",
        "            # Step 4: Generate answer\n",
        "            print(\"🤖 Generating answer...\")\n",
        "            answer = self.answer_generator.generate_answer(question, combined_context)\n",
        "\n",
        "            # Step 5: Calculate confidence score\n",
        "            if similarity_scores:\n",
        "                confidence = sum(similarity_scores) / len(similarity_scores)\n",
        "                confidence = max(0.0, min(1.0, confidence))  # Clamp between 0 and 1\n",
        "            else:\n",
        "                confidence = 0.0\n",
        "\n",
        "            result = {\n",
        "                'answer': answer,\n",
        "                'sources': sources,\n",
        "                'confidence': round(confidence, 3),\n",
        "                'context_length': len(combined_context),\n",
        "                'num_sources_used': len(sources)\n",
        "            }\n",
        "\n",
        "            print(f\"✅ Answer generated (confidence: {result['confidence']})\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing question: {str(e)}\")\n",
        "            return {\n",
        "                'answer': f\"❌ Error processing your question: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "    def _enhance_query(self, question: str) -> str:\n",
        "        \"\"\"Enhance the query for better retrieval\"\"\"\n",
        "        # Add context words that might help with retrieval\n",
        "        enhanced = question.lower()\n",
        "\n",
        "        # Add domain-specific terms that might help\n",
        "        if any(word in enhanced for word in ['agent', 'agents']):\n",
        "            enhanced += \" artificial intelligence autonomous system\"\n",
        "        elif any(word in enhanced for word in ['definition', 'define', 'what is']):\n",
        "            enhanced += \" definition explanation concept\"\n",
        "        elif any(word in enhanced for word in ['method', 'approach', 'technique']):\n",
        "            enhanced += \" methodology algorithm approach\"\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "    def _combine_context(self, context_parts: List[str], question: str) -> str:\n",
        "        \"\"\"Intelligently combine context parts\"\"\"\n",
        "        # Prioritize context based on question relevance\n",
        "        question_words = set(question.lower().split())\n",
        "\n",
        "        # Score and sort context parts\n",
        "        scored_contexts = []\n",
        "        for context in context_parts:\n",
        "            context_words = set(context.lower().split())\n",
        "            overlap = len(question_words.intersection(context_words))\n",
        "            scored_contexts.append((overlap, context))\n",
        "\n",
        "        # Sort by relevance score\n",
        "        scored_contexts.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "        # Combine top contexts\n",
        "        combined = \"\\n\\n\".join([context for _, context in scored_contexts])\n",
        "\n",
        "        # Truncate if too long\n",
        "        if len(combined) > 2000:\n",
        "            combined = combined[:2000] + \"...\"\n",
        "\n",
        "        return combined\n",
        "\n",
        "    def get_system_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current system status and statistics\"\"\"\n",
        "        base_status = {\n",
        "            'is_ready': self.is_ready,\n",
        "            'documents_info': self.documents_info,\n",
        "            'vector_store_stats': self.vector_manager.get_stats()\n",
        "        }\n",
        "\n",
        "        if self.is_ready:\n",
        "            base_status['model_info'] = {\n",
        "                'embedding_model': self.vector_manager.model_name,\n",
        "                'generation_model': self.answer_generator.model_name,\n",
        "                'gpu_available': torch.cuda.is_available()\n",
        "            }\n",
        "\n",
        "        return base_status"
      ],
      "metadata": {
        "id": "s9Gkxxjf4Xlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Workflow:**\n",
        "PDFs → Chunks → Embeddings → Vector Store → Query → Answer"
      ],
      "metadata": {
        "id": "G5cCek5xONyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 7: File Helpers**\n",
        "**Colab-specific utilities:**"
      ],
      "metadata": {
        "id": "IVApw9b-OXrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def upload_pdf_files():\n",
        "    \"\"\"Helper function to upload PDF files in Colab\"\"\"\n",
        "    print(\"📤 Please upload your PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_paths = []\n",
        "    for filename, data in uploaded.items():\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Save file to current directory\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(data)\n",
        "            pdf_paths.append(filename)\n",
        "            print(f\"✅ Saved: {filename}\")\n",
        "        else:\n",
        "            print(f\"⚠️ Skipped non-PDF file: {filename}\")\n",
        "\n",
        "    return pdf_paths\n",
        "\n",
        "def setup_with_sample_data():\n",
        "    \"\"\"Setup system with sample data for demonstration\"\"\"\n",
        "    print(\"🎯 Setting up with sample data...\")\n",
        "    print(\"Note: In real usage, upload your own PDF files using upload_pdf_files()\")\n",
        "\n",
        "    # Create a sample document for demonstration\n",
        "    sample_content = \"\"\"\n",
        "    This is a sample AI research paper about Natural Language Processing.\n",
        "\n",
        "    Abstract: This paper presents a novel approach to question answering systems\n",
        "    using retrieval-augmented generation. Our method combines semantic search\n",
        "    with large language models to provide accurate and contextual answers.\n",
        "\n",
        "    Introduction: Question answering has been a fundamental challenge in NLP.\n",
        "    Recent advances in transformer models have shown promising results.\n",
        "\n",
        "    Methodology: We propose a hybrid approach that uses vector embeddings\n",
        "    for document retrieval and generative models for answer synthesis.\n",
        "\n",
        "    Results: Our system achieves 85% accuracy on benchmark datasets,\n",
        "    outperforming traditional keyword-based approaches by 20%.\n",
        "\n",
        "    Conclusion: The combination of retrieval and generation provides\n",
        "    superior performance for domain-specific question answering tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create sample PDF content (simplified)\n",
        "    from langchain.schema import Document\n",
        "\n",
        "    sample_doc = Document(\n",
        "        page_content=sample_content,\n",
        "        metadata={\n",
        "            'source_file': 'sample_paper.pdf',\n",
        "            'page_number': 1,\n",
        "            'total_pages': 1\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Process sample document\n",
        "    chunks = rag_system.doc_processor.create_chunks([sample_doc])\n",
        "\n",
        "    # Create vector store\n",
        "    if rag_system.vector_manager.create_vector_store(chunks):\n",
        "        rag_system.is_ready = True\n",
        "        rag_system.documents_info = {\n",
        "            'total_documents': 1,\n",
        "            'total_chunks': len(chunks),\n",
        "            'pdf_files': ['sample_paper.pdf'],\n",
        "            'setup_complete': True\n",
        "        }\n",
        "        print(\"✅ Sample system ready for testing!\")\n",
        "        return True\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "CoVbpZAR4YsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage Tips:**\n",
        "\n",
        "*   Drag-and-drop PDF upload support\n",
        "\n",
        "*   Sample data for quick testing"
      ],
      "metadata": {
        "id": "l8rwC4CDOf1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 8: Gradio UI**\n",
        "**Builds interactive interface:**"
      ],
      "metadata": {
        "id": "8AG6HkOHOk0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create an interactive web interface\"\"\"\n",
        "\n",
        "    def handle_file_upload(files):\n",
        "        if not files:\n",
        "            return \"❌ Please upload at least one PDF file.\"\n",
        "\n",
        "        try:\n",
        "            pdf_paths = []\n",
        "            for file in files:\n",
        "                if file.name.endswith('.pdf'):\n",
        "                    pdf_paths.append(file.name)\n",
        "\n",
        "            if not pdf_paths:\n",
        "                return \"❌ No valid PDF files found.\"\n",
        "\n",
        "            # Setup the RAG system\n",
        "            success = rag_system.setup(pdf_paths)\n",
        "\n",
        "            if success:\n",
        "                status = rag_system.get_system_status()\n",
        "                return f\"\"\"✅ Setup Complete!\n",
        "\n",
        "📊 System Status:\n",
        "• Documents loaded: {status['documents_info']['total_documents']}\n",
        "• Text chunks created: {status['documents_info']['total_chunks']}\n",
        "• Files processed: {', '.join(status['documents_info']['pdf_files'])}\n",
        "• Vector embeddings: {status['vector_store_stats']['total_vectors']}\n",
        "\n",
        "🎯 Ready to answer questions!\"\"\"\n",
        "            else:\n",
        "                return \"❌ Setup failed. Please check your PDF files and try again.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error during setup: {str(e)}\"\n",
        "\n",
        "    def handle_question(question):\n",
        "        if not question.strip():\n",
        "            return \"❌ Please enter a question.\", \"\"\n",
        "\n",
        "        result = rag_system.ask_question(question)\n",
        "\n",
        "        # Format answer\n",
        "        answer_text = f\"**Answer:** {result['answer']}\\n\\n\"\n",
        "        answer_text += f\"**Confidence:** {result['confidence']}/1.0\\n\\n\"\n",
        "\n",
        "        # Format sources\n",
        "        sources_text = \"**Sources:**\\n\\n\"\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            sources_text += f\"**{i}. {source['file']}** (Page {source['page']})\\n\"\n",
        "            sources_text += f\"   • Similarity: {source['similarity_score']}\\n\"\n",
        "            sources_text += f\"   • Preview: {source['content_preview']}\\n\\n\"\n",
        "\n",
        "        if not result['sources']:\n",
        "            sources_text = \"No sources found.\"\n",
        "\n",
        "        return answer_text, sources_text\n",
        "\n",
        "    def show_system_info():\n",
        "        status = rag_system.get_system_status()\n",
        "\n",
        "        if not status['is_ready']:\n",
        "            return \"System not ready. Please upload documents first.\"\n",
        "\n",
        "        info_text = f\"\"\"**System Information:**\n",
        "\n",
        "**Status:** {'✅ Ready' if status['is_ready'] else '❌ Not Ready'}\n",
        "\n",
        "**Documents:**\n",
        "• Total documents: {status['documents_info']['total_documents']}\n",
        "• Total chunks: {status['documents_info']['total_chunks']}\n",
        "• Files: {', '.join(status['documents_info']['pdf_files'])}\n",
        "\n",
        "**Vector Store:**\n",
        "• Total vectors: {status['vector_store_stats']['total_vectors']}\n",
        "• Vector dimension: {status['vector_store_stats']['vector_dimension']}\n",
        "• Embedding model: {status['vector_store_stats']['model_name']}\n",
        "\n",
        "**Models:**\n",
        "• Generation model: {status.get('model_info', {}).get('generation_model', 'N/A')}\n",
        "• GPU available: {status.get('model_info', {}).get('gpu_available', False)}\n",
        "\"\"\"\n",
        "        return info_text\n",
        "\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(\n",
        "        title=\"RAG System - AI Research Papers QA\",\n",
        "        theme=gr.themes.Soft()\n",
        "    ) as interface:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        # 🤖 RAG System: AI Research Papers Q&A\n",
        "\n",
        "        **Upload your AI research papers and ask questions about them!**\n",
        "\n",
        "        This system uses Retrieval-Augmented Generation to:\n",
        "        - 📖 Process and understand your research papers\n",
        "        - 🔍 Find relevant information for your questions\n",
        "        - 🤖 Generate accurate answers with source citations\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"📤 Upload & Setup\"):\n",
        "            gr.Markdown(\"### Step 1: Upload Your PDF Research Papers\")\n",
        "\n",
        "            file_upload = gr.File(\n",
        "                label=\"Select PDF Files\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\"],\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            setup_btn = gr.Button(\"🚀 Process Documents\", variant=\"primary\", size=\"lg\")\n",
        "            setup_output = gr.Textbox(\n",
        "                label=\"Setup Status\",\n",
        "                lines=10,\n",
        "                interactive=False,\n",
        "                placeholder=\"Upload PDFs and click 'Process Documents' to begin...\"\n",
        "            )\n",
        "\n",
        "            # Sample data button for demo\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### Or Try with Sample Data\")\n",
        "            sample_btn = gr.Button(\"🎯 Use Sample Data (Demo)\", variant=\"secondary\")\n",
        "\n",
        "            def setup_sample():\n",
        "                success = setup_with_sample_data()\n",
        "                if success:\n",
        "                    status = rag_system.get_system_status()\n",
        "                    return f\"\"\"✅ Sample System Ready!\n",
        "\n",
        "This is a demo with sample AI research paper content.\n",
        "You can now ask questions like:\n",
        "• \"What is the main contribution of this paper?\"\n",
        "• \"What methodology was used?\"\n",
        "• \"What were the results?\"\n",
        "\n",
        "📊 System loaded with {status['documents_info']['total_chunks']} text chunks.\"\"\"\n",
        "                else:\n",
        "                    return \"❌ Failed to setup sample data.\"\n",
        "\n",
        "            sample_btn.click(setup_sample, outputs=[setup_output])\n",
        "            setup_btn.click(handle_file_upload, inputs=[file_upload], outputs=[setup_output])\n",
        "\n",
        "        with gr.Tab(\"❓ Ask Questions\"):\n",
        "            gr.Markdown(\"### Ask Questions About Your Research Papers\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=4):\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"Your Question\",\n",
        "                        placeholder=\"e.g., What are the main contributions of this research?\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                with gr.Column(scale=1):\n",
        "                    ask_btn = gr.Button(\"🔍 Get Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    answer_output = gr.Textbox(\n",
        "                        label=\"Answer\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    sources_output = gr.Textbox(\n",
        "                        label=\"Sources & Citations\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            # Sample questions\n",
        "            gr.Markdown(\"### 💡 Sample Questions to Try:\")\n",
        "            sample_questions = [\n",
        "                \"What are the main contributions of this research?\",\n",
        "                \"What methodology was used in this study?\",\n",
        "                \"What are the key findings and results?\",\n",
        "                \"What are the limitations mentioned?\",\n",
        "                \"What future work is suggested?\"\n",
        "            ]\n",
        "\n",
        "            for q in sample_questions:\n",
        "                sample_q_btn = gr.Button(f\"📝 {q}\", variant=\"secondary\", size=\"sm\")\n",
        "                sample_q_btn.click(lambda x=q: x, outputs=[question_input])\n",
        "\n",
        "            ask_btn.click(\n",
        "                handle_question,\n",
        "                inputs=[question_input],\n",
        "                outputs=[answer_output, sources_output]\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"ℹ️ System Info\"):\n",
        "            gr.Markdown(\"### System Status and Information\")\n",
        "\n",
        "            info_btn = gr.Button(\"🔄 Refresh System Info\", variant=\"primary\")\n",
        "            info_output = gr.Textbox(\n",
        "                label=\"System Information\",\n",
        "                lines=15,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            info_btn.click(show_system_info, outputs=[info_output])\n",
        "\n",
        "            # Auto-load info on tab open\n",
        "            interface.load(show_system_info, outputs=[info_output])\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "print(\"\\n🎨 Creating Gradio Interface...\")\n",
        "interface = create_gradio_interface()\n",
        "print(\"✅ Interface ready!\")"
      ],
      "metadata": {
        "id": "2mwlIx2d4pfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interface Features:**\n",
        "\n",
        "*   Document upload/status panel\n",
        "\n",
        "*   Q&A with source citations\n",
        "\n",
        "*   System diagnostics view"
      ],
      "metadata": {
        "id": "oLAcDrFkOrmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 9: Runtime Controls**\n",
        "**Launch commands:**"
      ],
      "metadata": {
        "id": "FEV5wiaSOu0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_system():\n",
        "    \"\"\"Launch the complete system\"\"\"\n",
        "    print(\"🚀 Launching RAG System...\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"📋 Instructions:\")\n",
        "    print(\"1. Run this cell to launch the web interface\")\n",
        "    print(\"2. Upload your PDF research papers in the 'Upload & Setup' tab\")\n",
        "    print(\"3. Wait for processing to complete\")\n",
        "    print(\"4. Go to 'Ask Questions' tab and start asking!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Launch interface\n",
        "    interface.launch(\n",
        "        share=True,  # Creates a public shareable link\n",
        "        debug=True,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "# Quick test function\n",
        "def quick_test():\n",
        "    \"\"\"Quick test with sample data\"\"\"\n",
        "    print(\"🧪 Quick Test Mode\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Setup with sample data\n",
        "    if setup_with_sample_data():\n",
        "        print(\"\\n✅ Sample system ready!\")\n",
        "\n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            \"What is this paper about?\",\n",
        "            \"What methodology was used?\",\n",
        "            \"What were the main results?\"\n",
        "        ]\n",
        "\n",
        "        for question in test_questions:\n",
        "            print(f\"\\n❓ Question: {question}\")\n",
        "            result = rag_system.ask_question(question, num_sources=2)\n",
        "            print(f\"✅ Answer: {result['answer'][:200]}...\")\n",
        "            print(f\"📚 Sources: {len(result['sources'])} found\")\n",
        "            print(f\"🎯 Confidence: {result['confidence']}\")\n",
        "    else:\n",
        "        print(\"❌ Test setup failed\")"
      ],
      "metadata": {
        "id": "ttceAWcx_UWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two Usage Modes:**\n",
        "\n",
        "*   Full web interface (run_system())\n",
        "\n",
        "*   CLI-style testing (quick_test())"
      ],
      "metadata": {
        "id": "GhZIl7VbO0RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_system()"
      ],
      "metadata": {
        "id": "mUlgm2hB5Hov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}