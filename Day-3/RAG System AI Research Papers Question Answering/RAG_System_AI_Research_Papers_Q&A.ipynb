{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2R1q+jJRfTUGJamjyuu0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnupriya-Selvraj/Agentic_AI_Workshop/blob/main/Day-3/RAG%20System%20AI%20Research%20Papers%20Question%20Answering/RAG_System_AI_Research_Papers_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 1: Installation and Setup**\n",
        "\n",
        "**Installs core dependencies:**"
      ],
      "metadata": {
        "id": "1rpCV7XcMtb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-huggingface\n",
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install pypdf2 PyPDF2\n",
        "!pip install transformers torch\n",
        "!pip install gradio\n",
        "!pip install accelerate\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ],
      "metadata": {
        "id": "-Y5dODOv3x2e",
        "outputId": "d60a8f2f-0efd-4a2e-f917-7d78b2c4c624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.34.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-huggingface, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-huggingface-0.3.1 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.6/211.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Packages:**\n",
        "\n",
        "*  langchain - Framework for building RAG\n",
        "systems\n",
        "\n",
        "*   sentence-transformers - For generating text embeddings\n",
        "\n",
        "*   faiss-cpu - Efficient vector similarity search\n",
        "\n",
        "*   PyPDF2 - PDF text extraction\n",
        "\n",
        "*   transformers - Language models for answer generation\n",
        "\n",
        "*   gradio - Web interface builder"
      ],
      "metadata": {
        "id": "YE1elSOrM34x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 2: Import Libraries**\n",
        "**Loads all required modules:**"
      ],
      "metadata": {
        "id": "StTN9anGNZoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from typing import List, Dict, Any\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üî• GPU Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "uVLxq41332aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Critical Components:**\n",
        "\n",
        "*   PDF processing tools (PyPDFLoader)\n",
        "\n",
        "*   Embedding models (HuggingFaceEmbeddings)\n",
        "\n",
        "*   Vector database (FAISS)\n",
        "\n",
        "*   GPU check (torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "9esdukj_Niv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 3: Document Processor**\n",
        "**Handles PDF loading and preprocessing:**"
      ],
      "metadata": {
        "id": "jOoAoYO_Nrkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"Handles loading and processing of PDF documents\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=1000, chunk_overlap=200):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def load_pdfs(self, pdf_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"Load multiple PDF files and return documents\"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            try:\n",
        "                print(f\"üìñ Loading: {os.path.basename(pdf_path)}\")\n",
        "                loader = PyPDFLoader(pdf_path)\n",
        "                documents = loader.load()\n",
        "\n",
        "                # Add metadata\n",
        "                for i, doc in enumerate(documents):\n",
        "                    doc.metadata.update({\n",
        "                        'source_file': os.path.basename(pdf_path),\n",
        "                        'page_number': i + 1,\n",
        "                        'total_pages': len(documents)\n",
        "                    })\n",
        "\n",
        "                all_documents.extend(documents)\n",
        "                print(f\"‚úÖ Loaded {len(documents)} pages from {os.path.basename(pdf_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading {pdf_path}: {str(e)}\")\n",
        "\n",
        "        return all_documents\n",
        "\n",
        "    def create_chunks(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for better retrieval\"\"\"\n",
        "        print(\"üî™ Creating text chunks...\")\n",
        "\n",
        "        chunks = self.text_splitter.split_documents(documents)\n",
        "\n",
        "        # Add chunk metadata\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata.update({\n",
        "                'chunk_id': i,\n",
        "                'chunk_size': len(chunk.page_content),\n",
        "                'processing_timestamp': str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "        return chunks\n",
        "\n",
        "# Test the document processor\n",
        "print(\"\\nüß™ Testing Document Processor...\")\n",
        "doc_processor = DocumentProcessor()\n",
        "print(\"Document processor ready!\")"
      ],
      "metadata": {
        "id": "9aIVP-nR3-0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Features:**\n",
        "\n",
        "*   Chunk size customization (default: 1000 chars)\n",
        "\n",
        "*   Metadata preservation (source file, page numbers)\n",
        "\n",
        "*   Smart text splitting at natural boundaries"
      ],
      "metadata": {
        "id": "213IVi8GNyum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 4: Vector Store Manager**\n",
        "**Manages document embeddings:**"
      ],
      "metadata": {
        "id": "KYJUE3fQN1zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStoreManager:\n",
        "    \"\"\"Manages document embeddings and similarity search\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "            encode_kwargs={'normalize_embeddings': True}\n",
        "        )\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, chunks: List[Document]) -> bool:\n",
        "        \"\"\"Create FAISS vector store from document chunks\"\"\"\n",
        "        try:\n",
        "            print(\"‚ö° Creating vector embeddings...\")\n",
        "            print(f\"üìä Processing {len(chunks)} chunks...\")\n",
        "\n",
        "            # Create embeddings and vector store\n",
        "            self.vector_store = FAISS.from_documents(\n",
        "                documents=chunks,\n",
        "                embedding=self.embeddings\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ Vector store created successfully!\")\n",
        "            print(f\"üìà Vector dimension: {self.vector_store.index.d}\")\n",
        "            print(f\"üìö Total vectors: {self.vector_store.index.ntotal}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error creating vector store: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def search_similar(self, query: str, k: int = 5) -> List[tuple]:\n",
        "        \"\"\"Search for similar documents\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Get documents with similarity scores\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "            print(f\"üîç Found {len(results)} relevant chunks for query\")\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Search error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get vector store statistics\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return {\"status\": \"not_created\"}\n",
        "\n",
        "        return {\n",
        "            \"status\": \"ready\",\n",
        "            \"total_vectors\": self.vector_store.index.ntotal,\n",
        "            \"vector_dimension\": self.vector_store.index.d,\n",
        "            \"model_name\": self.model_name\n",
        "        }\n",
        "\n",
        "# Test vector store manager\n",
        "print(\"\\nüß™ Testing Vector Store Manager...\")\n",
        "vector_manager = VectorStoreManager()\n",
        "print(\"Vector store manager ready!\")"
      ],
      "metadata": {
        "id": "0GDZpqx34KfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Technical Details:**\n",
        "\n",
        "*   Uses **`all-MiniLM-L6-v2`** embedding model\n",
        "\n",
        "*   Automatic GPU/CPU switching\n",
        "\n",
        "*   Returns similarity scores with results"
      ],
      "metadata": {
        "id": "G5Di-qs6N5u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 5: Answer Generator**\n",
        "**Produces human-readable answers:**"
      ],
      "metadata": {
        "id": "oXoEw8-ON_76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerGenerator:\n",
        "    \"\"\"Generates answers using retrieved context\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.model_name = \"microsoft/DialoGPT-medium\"\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Initialize the text generation model\"\"\"\n",
        "        try:\n",
        "            print(\"ü§ñ Loading language model...\")\n",
        "\n",
        "            # Use a simple but effective model for answer generation\n",
        "            self.generator = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=self.model_name,\n",
        "                tokenizer=self.model_name,\n",
        "                max_length=512,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=50256,\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ Language model loaded!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model: {str(e)}\")\n",
        "            # Fallback to a simpler approach\n",
        "            self.generator = None\n",
        "\n",
        "    def generate_answer(self, question: str, context: str, max_length: int = 200) -> str:\n",
        "        \"\"\"Generate answer based on context\"\"\"\n",
        "\n",
        "        if not self.generator:\n",
        "            return self._simple_answer(question, context)\n",
        "\n",
        "        try:\n",
        "            # Create a focused prompt\n",
        "            prompt = f\"\"\"\n",
        "Based on the research paper context below, answer the question concisely and accurately.\n",
        "\n",
        "Context: {context[:1000]}...\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "            # Generate response\n",
        "            response = self.generator(\n",
        "                prompt,\n",
        "                max_length=len(prompt) + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.3,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "            # Extract just the answer part\n",
        "            full_response = response[0]['generated_text']\n",
        "            answer = full_response[len(prompt):].strip()\n",
        "\n",
        "            # Clean up the answer\n",
        "            answer = self._clean_answer(answer)\n",
        "\n",
        "            return answer if answer else self._simple_answer(question, context)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Generation error: {str(e)}\")\n",
        "            return self._simple_answer(question, context)\n",
        "\n",
        "    def _simple_answer(self, question: str, context: str) -> str:\n",
        "        \"\"\"Fallback method for answer generation\"\"\"\n",
        "        # Simple extractive approach\n",
        "        sentences = context.split('.')\n",
        "        relevant_sentences = []\n",
        "\n",
        "        question_words = set(question.lower().split())\n",
        "\n",
        "        for sentence in sentences[:5]:  # Take first 5 sentences\n",
        "            sentence_words = set(sentence.lower().split())\n",
        "            if question_words.intersection(sentence_words):\n",
        "                relevant_sentences.append(sentence.strip())\n",
        "\n",
        "        if relevant_sentences:\n",
        "            return \". \".join(relevant_sentences[:2]) + \".\"\n",
        "        else:\n",
        "            return \"Based on the provided context, \" + sentences[0][:200] + \"...\"\n",
        "\n",
        "    def _clean_answer(self, answer: str) -> str:\n",
        "        \"\"\"Clean and format the generated answer\"\"\"\n",
        "        # Remove common artifacts\n",
        "        answer = answer.replace('\\n', ' ')\n",
        "        answer = ' '.join(answer.split())  # Remove extra spaces\n",
        "\n",
        "        # Ensure it ends properly\n",
        "        if answer and not answer.endswith(('.', '!', '?')):\n",
        "            answer += '.'\n",
        "\n",
        "        return answer\n",
        "\n",
        "# Test answer generator\n",
        "print(\"\\nüß™ Testing Answer Generator...\")\n",
        "answer_gen = AnswerGenerator()\n",
        "print(\"Answer generator ready!\")\n"
      ],
      "metadata": {
        "id": "mOLQ-LoV4M6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generation Pipeline:**\n",
        "\n",
        "*   Creates LLM prompt with question + context\n",
        "\n",
        "*   Uses DialoGPT-medium for generation\n",
        "\n",
        "*   Fallback to extractive QA if generation fails"
      ],
      "metadata": {
        "id": "ry0MyXxSOF1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 6: RAG System**\n",
        "**Main orchestrator class:**"
      ],
      "metadata": {
        "id": "F9KjDwa5OJYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete Retrieval-Augmented Generation system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.doc_processor = DocumentProcessor()\n",
        "        self.vector_manager = VectorStoreManager()\n",
        "        self.answer_generator = AnswerGenerator()\n",
        "        self.is_ready = False\n",
        "        self.documents_info = {}\n",
        "\n",
        "    def setup(self, pdf_paths: List[str]) -> bool:\n",
        "        \"\"\"Complete system setup\"\"\"\n",
        "        print(\"üöÄ Setting up RAG system...\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load documents\n",
        "            print(\"Step 1: Loading PDFs...\")\n",
        "            documents = self.doc_processor.load_pdfs(pdf_paths)\n",
        "\n",
        "            if not documents:\n",
        "                print(\"‚ùå No documents loaded!\")\n",
        "                return False\n",
        "\n",
        "            # Step 2: Create chunks\n",
        "            print(\"\\nStep 2: Creating text chunks...\")\n",
        "            chunks = self.doc_processor.create_chunks(documents)\n",
        "\n",
        "            # Step 3: Create vector store\n",
        "            print(\"\\nStep 3: Creating vector embeddings...\")\n",
        "            if not self.vector_manager.create_vector_store(chunks):\n",
        "                return False\n",
        "\n",
        "            # Step 4: Store document information\n",
        "            self.documents_info = {\n",
        "                'total_documents': len(documents),\n",
        "                'total_chunks': len(chunks),\n",
        "                'pdf_files': [os.path.basename(path) for path in pdf_paths],\n",
        "                'setup_complete': True\n",
        "            }\n",
        "\n",
        "            self.is_ready = True\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"üéâ RAG System Setup Complete!\")\n",
        "            print(f\"üìö Loaded: {len(documents)} pages from {len(pdf_paths)} PDFs\")\n",
        "            print(f\"üî™ Created: {len(chunks)} text chunks\")\n",
        "            print(f\"‚ö° Vector store ready with {self.vector_manager.vector_store.index.ntotal} embeddings\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Setup failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def ask_question(self, question: str, num_sources: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Ask a question and get an answer with sources\"\"\"\n",
        "\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                'answer': \"‚ùå System not ready. Please upload and process documents first.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        if not question.strip():\n",
        "            return {\n",
        "                'answer': \"‚ùå Please provide a valid question.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"üîç Processing question: '{question}'\")\n",
        "\n",
        "            # Step 1: Retrieve relevant documents\n",
        "            search_results = self.vector_manager.search_similar(question, k=num_sources)\n",
        "\n",
        "            if not search_results:\n",
        "                return {\n",
        "                    'answer': \"‚ùå No relevant information found in the documents.\",\n",
        "                    'sources': [],\n",
        "                    'confidence': 0.0\n",
        "                }\n",
        "\n",
        "            # Step 2: Prepare context and source information\n",
        "            context_parts = []\n",
        "            sources = []\n",
        "            total_score = 0\n",
        "\n",
        "            for doc, score in search_results:\n",
        "                context_parts.append(doc.page_content)\n",
        "                total_score += (1 - score)  # Convert distance to similarity\n",
        "\n",
        "                source_info = {\n",
        "                    'file': doc.metadata.get('source_file', 'Unknown'),\n",
        "                    'page': doc.metadata.get('page_number', 'Unknown'),\n",
        "                    'chunk_id': doc.metadata.get('chunk_id', 'Unknown'),\n",
        "                    'similarity_score': round(1 - score, 3),  # Convert to similarity\n",
        "                    'content_preview': doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
        "                }\n",
        "                sources.append(source_info)\n",
        "\n",
        "            # Step 3: Combine context\n",
        "            combined_context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "            # Step 4: Generate answer\n",
        "            print(\"ü§ñ Generating answer...\")\n",
        "            answer = self.answer_generator.generate_answer(question, combined_context)\n",
        "\n",
        "            # Step 5: Calculate confidence score\n",
        "            confidence = min(total_score / len(search_results), 1.0)\n",
        "\n",
        "            result = {\n",
        "                'answer': answer,\n",
        "                'sources': sources,\n",
        "                'confidence': round(confidence, 3),\n",
        "                'context_length': len(combined_context),\n",
        "                'num_sources_used': len(sources)\n",
        "            }\n",
        "\n",
        "            print(f\"‚úÖ Answer generated (confidence: {result['confidence']})\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing question: {str(e)}\")\n",
        "            return {\n",
        "                'answer': f\"‚ùå Error processing your question: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "    def get_system_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current system status and statistics\"\"\"\n",
        "        base_status = {\n",
        "            'is_ready': self.is_ready,\n",
        "            'documents_info': self.documents_info,\n",
        "            'vector_store_stats': self.vector_manager.get_stats()\n",
        "        }\n",
        "\n",
        "        if self.is_ready:\n",
        "            base_status['model_info'] = {\n",
        "                'embedding_model': self.vector_manager.model_name,\n",
        "                'generation_model': self.answer_generator.model_name,\n",
        "                'gpu_available': torch.cuda.is_available()\n",
        "            }\n",
        "\n",
        "        return base_status\n",
        "\n",
        "# Initialize the complete RAG system\n",
        "print(\"\\nüß™ Initializing Complete RAG System...\")\n",
        "rag_system = RAGSystem()\n",
        "print(\"‚úÖ RAG system ready for setup!\")\n"
      ],
      "metadata": {
        "id": "s9Gkxxjf4Xlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Workflow:**\n",
        "PDFs ‚Üí Chunks ‚Üí Embeddings ‚Üí Vector Store ‚Üí Query ‚Üí Answer"
      ],
      "metadata": {
        "id": "G5cCek5xONyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 7: File Helpers**\n",
        "**Colab-specific utilities:**"
      ],
      "metadata": {
        "id": "IVApw9b-OXrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def upload_pdf_files():\n",
        "    \"\"\"Helper function to upload PDF files in Colab\"\"\"\n",
        "    print(\"üì§ Please upload your PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_paths = []\n",
        "    for filename, data in uploaded.items():\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Save file to current directory\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(data)\n",
        "            pdf_paths.append(filename)\n",
        "            print(f\"‚úÖ Saved: {filename}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Skipped non-PDF file: {filename}\")\n",
        "\n",
        "    return pdf_paths\n",
        "\n",
        "def setup_with_sample_data():\n",
        "    \"\"\"Setup system with sample data for demonstration\"\"\"\n",
        "    print(\"üéØ Setting up with sample data...\")\n",
        "    print(\"Note: In real usage, upload your own PDF files using upload_pdf_files()\")\n",
        "\n",
        "    # Create a sample document for demonstration\n",
        "    sample_content = \"\"\"\n",
        "    This is a sample AI research paper about Natural Language Processing.\n",
        "\n",
        "    Abstract: This paper presents a novel approach to question answering systems\n",
        "    using retrieval-augmented generation. Our method combines semantic search\n",
        "    with large language models to provide accurate and contextual answers.\n",
        "\n",
        "    Introduction: Question answering has been a fundamental challenge in NLP.\n",
        "    Recent advances in transformer models have shown promising results.\n",
        "\n",
        "    Methodology: We propose a hybrid approach that uses vector embeddings\n",
        "    for document retrieval and generative models for answer synthesis.\n",
        "\n",
        "    Results: Our system achieves 85% accuracy on benchmark datasets,\n",
        "    outperforming traditional keyword-based approaches by 20%.\n",
        "\n",
        "    Conclusion: The combination of retrieval and generation provides\n",
        "    superior performance for domain-specific question answering tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create sample PDF content (simplified)\n",
        "    from langchain.schema import Document\n",
        "\n",
        "    sample_doc = Document(\n",
        "        page_content=sample_content,\n",
        "        metadata={\n",
        "            'source_file': 'sample_paper.pdf',\n",
        "            'page_number': 1,\n",
        "            'total_pages': 1\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Process sample document\n",
        "    chunks = rag_system.doc_processor.create_chunks([sample_doc])\n",
        "\n",
        "    # Create vector store\n",
        "    if rag_system.vector_manager.create_vector_store(chunks):\n",
        "        rag_system.is_ready = True\n",
        "        rag_system.documents_info = {\n",
        "            'total_documents': 1,\n",
        "            'total_chunks': len(chunks),\n",
        "            'pdf_files': ['sample_paper.pdf'],\n",
        "            'setup_complete': True\n",
        "        }\n",
        "        print(\"‚úÖ Sample system ready for testing!\")\n",
        "        return True\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "CoVbpZAR4YsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage Tips:**\n",
        "\n",
        "*   Drag-and-drop PDF upload support\n",
        "\n",
        "*   Sample data for quick testing"
      ],
      "metadata": {
        "id": "l8rwC4CDOf1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 8: Gradio UI**\n",
        "**Builds interactive interface:**"
      ],
      "metadata": {
        "id": "8AG6HkOHOk0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create an interactive web interface\"\"\"\n",
        "\n",
        "    def handle_file_upload(files):\n",
        "        if not files:\n",
        "            return \"‚ùå Please upload at least one PDF file.\"\n",
        "\n",
        "        try:\n",
        "            pdf_paths = []\n",
        "            for file in files:\n",
        "                if file.name.endswith('.pdf'):\n",
        "                    pdf_paths.append(file.name)\n",
        "\n",
        "            if not pdf_paths:\n",
        "                return \"‚ùå No valid PDF files found.\"\n",
        "\n",
        "            # Setup the RAG system\n",
        "            success = rag_system.setup(pdf_paths)\n",
        "\n",
        "            if success:\n",
        "                status = rag_system.get_system_status()\n",
        "                return f\"\"\"‚úÖ Setup Complete!\n",
        "\n",
        "üìä System Status:\n",
        "‚Ä¢ Documents loaded: {status['documents_info']['total_documents']}\n",
        "‚Ä¢ Text chunks created: {status['documents_info']['total_chunks']}\n",
        "‚Ä¢ Files processed: {', '.join(status['documents_info']['pdf_files'])}\n",
        "‚Ä¢ Vector embeddings: {status['vector_store_stats']['total_vectors']}\n",
        "\n",
        "üéØ Ready to answer questions!\"\"\"\n",
        "            else:\n",
        "                return \"‚ùå Setup failed. Please check your PDF files and try again.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error during setup: {str(e)}\"\n",
        "\n",
        "    def handle_question(question):\n",
        "        if not question.strip():\n",
        "            return \"‚ùå Please enter a question.\", \"\"\n",
        "\n",
        "        result = rag_system.ask_question(question)\n",
        "\n",
        "        # Format answer\n",
        "        answer_text = f\"**Answer:** {result['answer']}\\n\\n\"\n",
        "        answer_text += f\"**Confidence:** {result['confidence']}/1.0\\n\\n\"\n",
        "\n",
        "        # Format sources\n",
        "        sources_text = \"**Sources:**\\n\\n\"\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            sources_text += f\"**{i}. {source['file']}** (Page {source['page']})\\n\"\n",
        "            sources_text += f\"   ‚Ä¢ Similarity: {source['similarity_score']}\\n\"\n",
        "            sources_text += f\"   ‚Ä¢ Preview: {source['content_preview']}\\n\\n\"\n",
        "\n",
        "        if not result['sources']:\n",
        "            sources_text = \"No sources found.\"\n",
        "\n",
        "        return answer_text, sources_text\n",
        "\n",
        "    def show_system_info():\n",
        "        status = rag_system.get_system_status()\n",
        "\n",
        "        if not status['is_ready']:\n",
        "            return \"System not ready. Please upload documents first.\"\n",
        "\n",
        "        info_text = f\"\"\"**System Information:**\n",
        "\n",
        "**Status:** {'‚úÖ Ready' if status['is_ready'] else '‚ùå Not Ready'}\n",
        "\n",
        "**Documents:**\n",
        "‚Ä¢ Total documents: {status['documents_info']['total_documents']}\n",
        "‚Ä¢ Total chunks: {status['documents_info']['total_chunks']}\n",
        "‚Ä¢ Files: {', '.join(status['documents_info']['pdf_files'])}\n",
        "\n",
        "**Vector Store:**\n",
        "‚Ä¢ Total vectors: {status['vector_store_stats']['total_vectors']}\n",
        "‚Ä¢ Vector dimension: {status['vector_store_stats']['vector_dimension']}\n",
        "‚Ä¢ Embedding model: {status['vector_store_stats']['model_name']}\n",
        "\n",
        "**Models:**\n",
        "‚Ä¢ Generation model: {status.get('model_info', {}).get('generation_model', 'N/A')}\n",
        "‚Ä¢ GPU available: {status.get('model_info', {}).get('gpu_available', False)}\n",
        "\"\"\"\n",
        "        return info_text\n",
        "\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(\n",
        "        title=\"RAG System - AI Research Papers QA\",\n",
        "        theme=gr.themes.Soft()\n",
        "    ) as interface:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ü§ñ RAG System: AI Research Papers Q&A\n",
        "\n",
        "        **Upload your AI research papers and ask questions about them!**\n",
        "\n",
        "        This system uses Retrieval-Augmented Generation to:\n",
        "        - üìñ Process and understand your research papers\n",
        "        - üîç Find relevant information for your questions\n",
        "        - ü§ñ Generate accurate answers with source citations\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"üì§ Upload & Setup\"):\n",
        "            gr.Markdown(\"### Step 1: Upload Your PDF Research Papers\")\n",
        "\n",
        "            file_upload = gr.File(\n",
        "                label=\"Select PDF Files\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\"],\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            setup_btn = gr.Button(\"üöÄ Process Documents\", variant=\"primary\", size=\"lg\")\n",
        "            setup_output = gr.Textbox(\n",
        "                label=\"Setup Status\",\n",
        "                lines=10,\n",
        "                interactive=False,\n",
        "                placeholder=\"Upload PDFs and click 'Process Documents' to begin...\"\n",
        "            )\n",
        "\n",
        "            # Sample data button for demo\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### Or Try with Sample Data\")\n",
        "            sample_btn = gr.Button(\"üéØ Use Sample Data (Demo)\", variant=\"secondary\")\n",
        "\n",
        "            def setup_sample():\n",
        "                success = setup_with_sample_data()\n",
        "                if success:\n",
        "                    status = rag_system.get_system_status()\n",
        "                    return f\"\"\"‚úÖ Sample System Ready!\n",
        "\n",
        "This is a demo with sample AI research paper content.\n",
        "You can now ask questions like:\n",
        "‚Ä¢ \"What is the main contribution of this paper?\"\n",
        "‚Ä¢ \"What methodology was used?\"\n",
        "‚Ä¢ \"What were the results?\"\n",
        "\n",
        "üìä System loaded with {status['documents_info']['total_chunks']} text chunks.\"\"\"\n",
        "                else:\n",
        "                    return \"‚ùå Failed to setup sample data.\"\n",
        "\n",
        "            sample_btn.click(setup_sample, outputs=[setup_output])\n",
        "            setup_btn.click(handle_file_upload, inputs=[file_upload], outputs=[setup_output])\n",
        "\n",
        "        with gr.Tab(\"‚ùì Ask Questions\"):\n",
        "            gr.Markdown(\"### Ask Questions About Your Research Papers\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=4):\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"Your Question\",\n",
        "                        placeholder=\"e.g., What are the main contributions of this research?\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                with gr.Column(scale=1):\n",
        "                    ask_btn = gr.Button(\"üîç Get Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    answer_output = gr.Textbox(\n",
        "                        label=\"Answer\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    sources_output = gr.Textbox(\n",
        "                        label=\"Sources & Citations\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            # Sample questions\n",
        "            gr.Markdown(\"### üí° Sample Questions to Try:\")\n",
        "            sample_questions = [\n",
        "                \"What are the main contributions of this research?\",\n",
        "                \"What methodology was used in this study?\",\n",
        "                \"What are the key findings and results?\",\n",
        "                \"What are the limitations mentioned?\",\n",
        "                \"What future work is suggested?\"\n",
        "            ]\n",
        "\n",
        "            for q in sample_questions:\n",
        "                sample_q_btn = gr.Button(f\"üìù {q}\", variant=\"secondary\", size=\"sm\")\n",
        "                sample_q_btn.click(lambda x=q: x, outputs=[question_input])\n",
        "\n",
        "            ask_btn.click(\n",
        "                handle_question,\n",
        "                inputs=[question_input],\n",
        "                outputs=[answer_output, sources_output]\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"‚ÑπÔ∏è System Info\"):\n",
        "            gr.Markdown(\"### System Status and Information\")\n",
        "\n",
        "            info_btn = gr.Button(\"üîÑ Refresh System Info\", variant=\"primary\")\n",
        "            info_output = gr.Textbox(\n",
        "                label=\"System Information\",\n",
        "                lines=15,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            info_btn.click(show_system_info, outputs=[info_output])\n",
        "\n",
        "            # Auto-load info on tab open\n",
        "            interface.load(show_system_info, outputs=[info_output])\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "print(\"\\nüé® Creating Gradio Interface...\")\n",
        "interface = create_gradio_interface()\n",
        "print(\"‚úÖ Interface ready!\")"
      ],
      "metadata": {
        "id": "2mwlIx2d4pfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interface Features:**\n",
        "\n",
        "*   Document upload/status panel\n",
        "\n",
        "*   Q&A with source citations\n",
        "\n",
        "*   System diagnostics view"
      ],
      "metadata": {
        "id": "oLAcDrFkOrmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 9: Runtime Controls**\n",
        "**Launch commands:**"
      ],
      "metadata": {
        "id": "FEV5wiaSOu0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_system():\n",
        "    \"\"\"Launch the complete system\"\"\"\n",
        "    print(\"üöÄ Launching RAG System...\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"üìã Instructions:\")\n",
        "    print(\"1. Run this cell to launch the web interface\")\n",
        "    print(\"2. Upload your PDF research papers in the 'Upload & Setup' tab\")\n",
        "    print(\"3. Wait for processing to complete\")\n",
        "    print(\"4. Go to 'Ask Questions' tab and start asking!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Launch interface\n",
        "    interface.launch(\n",
        "        share=True,  # Creates a public shareable link\n",
        "        debug=True,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "# Quick test function\n",
        "def quick_test():\n",
        "    \"\"\"Quick test with sample data\"\"\"\n",
        "    print(\"üß™ Quick Test Mode\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Setup with sample data\n",
        "    if setup_with_sample_data():\n",
        "        print(\"\\n‚úÖ Sample system ready!\")\n",
        "\n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            \"What is this paper about?\",\n",
        "            \"What methodology was used?\",\n",
        "            \"What were the main results?\"\n",
        "        ]\n",
        "\n",
        "        for question in test_questions:\n",
        "            print(f\"\\n‚ùì Question: {question}\")\n",
        "            result = rag_system.ask_question(question, num_sources=2)\n",
        "            print(f\"‚úÖ Answer: {result['answer'][:200]}...\")\n",
        "            print(f\"üìö Sources: {len(result['sources'])} found\")\n",
        "            print(f\"üéØ Confidence: {result['confidence']}\")\n",
        "    else:\n",
        "        print(\"‚ùå Test setup failed\")"
      ],
      "metadata": {
        "id": "ttceAWcx_UWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two Usage Modes:**\n",
        "\n",
        "*   Full web interface (run_system())\n",
        "\n",
        "*   CLI-style testing (quick_test())"
      ],
      "metadata": {
        "id": "GhZIl7VbO0RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_system()"
      ],
      "metadata": {
        "id": "mUlgm2hB5Hov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}