{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNscuDqn8UIGANW4/SKTU2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnupriya-Selvraj/Agentic_AI_Workshop/blob/main/Day-3/RAG%20System%20AI%20Research%20Papers%20Question%20Answering/RAG_System_AI_Research_Papers_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 1: Installation and Setup**\n",
        "\n",
        "**Installs core dependencies:**"
      ],
      "metadata": {
        "id": "1rpCV7XcMtb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-huggingface\n",
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install pypdf2 PyPDF2\n",
        "!pip install transformers torch\n",
        "!pip install gradio\n",
        "!pip install accelerate\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")"
      ],
      "metadata": {
        "id": "-Y5dODOv3x2e",
        "outputId": "e76e0618-bc93-4aef-9163-41dd6ef87641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.34.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n",
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.34.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.14)\n",
            "âœ… All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Packages:**\n",
        "\n",
        "*  langchain - Framework for building RAG\n",
        "systems\n",
        "\n",
        "*   sentence-transformers - For generating text embeddings\n",
        "\n",
        "*   faiss-cpu - Efficient vector similarity search\n",
        "\n",
        "*   PyPDF2 - PDF text extraction\n",
        "\n",
        "*   transformers - Language models for answer generation\n",
        "\n",
        "*   gradio - Web interface builder"
      ],
      "metadata": {
        "id": "YE1elSOrM34x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 2: Import Libraries**\n",
        "**Loads all required modules:**"
      ],
      "metadata": {
        "id": "StTN9anGNZoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from typing import List, Dict, Any\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"ðŸ”¥ GPU Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "uVLxq41332aB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7809a51d-668a-4daf-98dc-dcc5eb99c18a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n",
            "ðŸ”¥ GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Critical Components:**\n",
        "\n",
        "*   PDF processing tools (PyPDFLoader)\n",
        "\n",
        "*   Embedding models (HuggingFaceEmbeddings)\n",
        "\n",
        "*   Vector database (FAISS)\n",
        "\n",
        "*   GPU check (torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "9esdukj_Niv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 3: Document Processor**\n",
        "**Handles PDF loading and preprocessing:**"
      ],
      "metadata": {
        "id": "jOoAoYO_Nrkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"Handles loading and processing of PDF documents with better chunking\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=100):  # Smaller chunks for better precision\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Better separators\n",
        "        )\n",
        "\n",
        "    def load_pdfs(self, pdf_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"Load multiple PDF files and return documents with better preprocessing\"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            try:\n",
        "                print(f\"ðŸ“– Loading: {os.path.basename(pdf_path)}\")\n",
        "                loader = PyPDFLoader(pdf_path)\n",
        "                documents = loader.load()\n",
        "\n",
        "                # Clean and preprocess documents\n",
        "                for i, doc in enumerate(documents):\n",
        "                    # Clean the text content\n",
        "                    cleaned_content = self._clean_text(doc.page_content)\n",
        "                    doc.page_content = cleaned_content\n",
        "\n",
        "                    # Add comprehensive metadata\n",
        "                    doc.metadata.update({\n",
        "                        'source_file': os.path.basename(pdf_path),\n",
        "                        'page_number': i + 1,\n",
        "                        'total_pages': len(documents),\n",
        "                        'char_count': len(cleaned_content)\n",
        "                    })\n",
        "\n",
        "                all_documents.extend(documents)\n",
        "                print(f\"âœ… Loaded {len(documents)} pages from {os.path.basename(pdf_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error loading {pdf_path}: {str(e)}\")\n",
        "\n",
        "        return all_documents\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text content\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove page numbers and headers/footers (simple heuristic)\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            # Skip very short lines that might be headers/footers\n",
        "            if len(line) > 10 and not re.match(r'^\\d+$', line):\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        return ' '.join(cleaned_lines)\n",
        "\n",
        "    def create_chunks(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks with better metadata\"\"\"\n",
        "        print(\"ðŸ”ª Creating text chunks...\")\n",
        "\n",
        "        chunks = self.text_splitter.split_documents(documents)\n",
        "\n",
        "        # Add enhanced chunk metadata\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata.update({\n",
        "                'chunk_id': i,\n",
        "                'chunk_size': len(chunk.page_content),\n",
        "                'word_count': len(chunk.page_content.split()),\n",
        "                'processing_timestamp': str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "        print(f\"âœ… Created {len(chunks)} chunks\")\n",
        "        return chunks\n"
      ],
      "metadata": {
        "id": "9aIVP-nR3-0D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Features:**\n",
        "\n",
        "*   Chunk size customization (default: 1000 chars)\n",
        "\n",
        "*   Metadata preservation (source file, page numbers)\n",
        "\n",
        "*   Smart text splitting at natural boundaries"
      ],
      "metadata": {
        "id": "213IVi8GNyum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 4: Vector Store Manager**\n",
        "**Manages document embeddings:**"
      ],
      "metadata": {
        "id": "KYJUE3fQN1zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStoreManager:\n",
        "    \"\"\"Manages document embeddings with better similarity search\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "            encode_kwargs={\n",
        "                'normalize_embeddings': True,\n",
        "                'batch_size': 16  # Better batch size for processing\n",
        "            }\n",
        "        )\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, chunks: List[Document]) -> bool:\n",
        "        \"\"\"Create FAISS vector store with better configuration\"\"\"\n",
        "        try:\n",
        "            print(\"âš¡ Creating vector embeddings...\")\n",
        "            print(f\"ðŸ“Š Processing {len(chunks)} chunks...\")\n",
        "\n",
        "            # Filter out very short chunks that might not be meaningful\n",
        "            filtered_chunks = [chunk for chunk in chunks if len(chunk.page_content.strip()) > 20]\n",
        "\n",
        "            print(f\"ðŸ“Š Using {len(filtered_chunks)} meaningful chunks...\")\n",
        "\n",
        "            # Create embeddings and vector store\n",
        "            self.vector_store = FAISS.from_documents(\n",
        "                documents=filtered_chunks,\n",
        "                embedding=self.embeddings\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… Vector store created successfully!\")\n",
        "            print(f\"ðŸ“ˆ Vector dimension: {self.vector_store.index.d}\")\n",
        "            print(f\"ðŸ“š Total vectors: {self.vector_store.index.ntotal}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error creating vector store: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def search_similar(self, query: str, k: int = 5) -> List[tuple]:\n",
        "        \"\"\"Search for similar documents with better scoring\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Use MMR (Maximum Marginal Relevance) for better diversity\n",
        "            try:\n",
        "                results = self.vector_store.max_marginal_relevance_search_with_score(\n",
        "                    query, k=k, fetch_k=k*2\n",
        "                )\n",
        "            except:\n",
        "                # Fallback to regular similarity search\n",
        "                results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "            print(f\"ðŸ” Found {len(results)} relevant chunks for query\")\n",
        "\n",
        "            # Filter results by minimum similarity threshold\n",
        "            filtered_results = []\n",
        "            for doc, score in results:\n",
        "                # Convert distance to similarity and filter low scores\n",
        "                similarity = 1 - score\n",
        "                if similarity > 0.1:  # Minimum threshold\n",
        "                    filtered_results.append((doc, score))\n",
        "\n",
        "            return filtered_results[:k]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Search error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get vector store statistics\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return {\"status\": \"not_created\"}\n",
        "\n",
        "        return {\n",
        "            \"status\": \"ready\",\n",
        "            \"total_vectors\": self.vector_store.index.ntotal,\n",
        "            \"vector_dimension\": self.vector_store.index.d,\n",
        "            \"model_name\": self.model_name\n",
        "        }"
      ],
      "metadata": {
        "id": "0GDZpqx34KfL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Technical Details:**\n",
        "\n",
        "*   Uses **`all-MiniLM-L6-v2`** embedding model\n",
        "\n",
        "*   Automatic GPU/CPU switching\n",
        "\n",
        "*   Returns similarity scores with results"
      ],
      "metadata": {
        "id": "G5Di-qs6N5u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 5: Answer Generator**\n",
        "**Produces human-readable answers:**"
      ],
      "metadata": {
        "id": "oXoEw8-ON_76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerGenerator:\n",
        "    \"\"\"Generates answers with significantly improved extraction logic\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.model_name = \"microsoft/DialoGPT-medium\"\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Initialize the text generation model\"\"\"\n",
        "        try:\n",
        "            print(\"ðŸ¤– Loading language model...\")\n",
        "            from transformers import pipeline\n",
        "            self.generator = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=\"microsoft/DialoGPT-medium\",\n",
        "                tokenizer=\"microsoft/DialoGPT-medium\",\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                pad_token_id=50256,\n",
        "                device=0 if torch.cuda.is_available() else -1,\n",
        "                truncation=True\n",
        "            )\n",
        "            print(\"âœ… Language model loaded!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading model: {str(e)}\")\n",
        "            self.generator = None\n",
        "\n",
        "    def generate_answer(self, question: str, context: str, max_length: int = 200) -> str:\n",
        "        \"\"\"Generate answer with advanced extraction techniques\"\"\"\n",
        "\n",
        "        # First, try definition extraction for \"what is\" questions\n",
        "        if self._is_definition_question(question):\n",
        "            definition_answer = self._extract_definition(question, context)\n",
        "            if definition_answer and len(definition_answer) > 20:\n",
        "                return definition_answer\n",
        "\n",
        "        # Try advanced extractive method\n",
        "        extractive_answer = self._advanced_extractive_answer(question, context)\n",
        "        if extractive_answer and len(extractive_answer) > 20:\n",
        "            return extractive_answer\n",
        "\n",
        "        # Try generative approach as fallback\n",
        "        if self.generator:\n",
        "            generative_answer = self._generate_with_model(question, context)\n",
        "            if generative_answer and len(generative_answer) > 20:\n",
        "                return generative_answer\n",
        "\n",
        "        # Final fallback\n",
        "        return self._simple_context_answer(context)\n",
        "\n",
        "    def _is_definition_question(self, question: str) -> bool:\n",
        "        \"\"\"Check if this is a definition question\"\"\"\n",
        "        definition_indicators = ['what is', 'define', 'definition of', 'what are', 'meaning of']\n",
        "        question_lower = question.lower()\n",
        "        return any(indicator in question_lower for indicator in definition_indicators)\n",
        "\n",
        "    def _extract_definition(self, question: str, context: str) -> str:\n",
        "        \"\"\"Extract definition-style answers\"\"\"\n",
        "        # Get the main term being defined\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Extract the key term\n",
        "        if 'what is' in question_lower:\n",
        "            term = question_lower.split('what is')[-1].strip('? .')\n",
        "        elif 'define' in question_lower:\n",
        "            term = question_lower.split('define')[-1].strip('? .')\n",
        "        else:\n",
        "            term = question_lower.replace('what are', '').replace('definition of', '').strip('? .')\n",
        "\n",
        "        # Clean up the term\n",
        "        term = term.split()[0] if term.split() else 'agent'\n",
        "\n",
        "        # Split context into sentences\n",
        "        sentences = [s.strip() for s in context.replace('\\n', ' ').split('.') if len(s.strip()) > 15]\n",
        "\n",
        "        # Look for definition patterns\n",
        "        definition_patterns = [\n",
        "            f\"{term} is\",\n",
        "            f\"{term} are\",\n",
        "            f\"an {term} is\",\n",
        "            f\"the {term} is\",\n",
        "            f\"{term} can be defined\",\n",
        "            f\"{term} refers to\",\n",
        "            f\"{term} means\",\n",
        "            \"agent is\",\n",
        "            \"agents are\",\n",
        "            \"an agent is\",\n",
        "            \"the agent is\"\n",
        "        ]\n",
        "\n",
        "        best_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_lower = sentence.lower()\n",
        "\n",
        "            # Check for definition patterns\n",
        "            for pattern in definition_patterns:\n",
        "                if pattern in sentence_lower:\n",
        "                    # This looks like a definition\n",
        "                    best_sentences.append((10, sentence))  # High priority\n",
        "                    break\n",
        "            else:\n",
        "                # Check for descriptive content about the term\n",
        "                if term in sentence_lower and len(sentence) > 30:\n",
        "                    # Count relevant keywords\n",
        "                    relevance_keywords = ['intelligence', 'autonomous', 'system', 'behavior', 'environment', 'action', 'goal', 'artificial']\n",
        "                    keyword_count = sum(1 for kw in relevance_keywords if kw in sentence_lower)\n",
        "                    if keyword_count > 0:\n",
        "                        best_sentences.append((keyword_count, sentence))\n",
        "\n",
        "        # Sort by relevance score\n",
        "        best_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "        if best_sentences:\n",
        "            # Take the best 2-3 sentences\n",
        "            selected_sentences = [sent[1] for sent in best_sentences[:3]]\n",
        "            answer = '. '.join(selected_sentences)\n",
        "            return self._clean_answer(answer)\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _advanced_extractive_answer(self, question: str, context: str) -> str:\n",
        "        \"\"\"Advanced extractive answer with better sentence selection\"\"\"\n",
        "\n",
        "        # Split into sentences and filter\n",
        "        sentences = []\n",
        "        for sent in context.replace('\\n', ' ').split('.'):\n",
        "            sent = sent.strip()\n",
        "            if len(sent) > 20 and not self._is_reference_sentence(sent):\n",
        "                sentences.append(sent)\n",
        "\n",
        "        if not sentences:\n",
        "            return \"\"\n",
        "\n",
        "        # Get question keywords\n",
        "        question_words = set(question.lower().split())\n",
        "        stop_words = {'what', 'is', 'are', 'how', 'why', 'when', 'where', 'who', 'which', 'the', 'a', 'an', 'of', 'in', 'to', 'for', 'and', 'or'}\n",
        "        key_words = question_words - stop_words\n",
        "\n",
        "        # Score sentences\n",
        "        scored_sentences = []\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_lower = sentence.lower()\n",
        "            sentence_words = set(sentence_lower.split())\n",
        "\n",
        "            # Calculate various scores\n",
        "            keyword_overlap = len(key_words.intersection(sentence_words))\n",
        "            position_score = 1.0 / (i + 1)  # Earlier sentences preferred\n",
        "            length_score = min(len(sentence) / 100, 1.0)  # Moderate length preferred\n",
        "\n",
        "            # Bonus for informative content\n",
        "            info_bonus = 0\n",
        "            info_words = ['intelligence', 'system', 'autonomous', 'behavior', 'environment', 'action', 'goal', 'artificial', 'capability', 'function', 'perform', 'task']\n",
        "            info_bonus = sum(0.5 for word in info_words if word in sentence_lower)\n",
        "\n",
        "            # Penalty for references and citations\n",
        "            citation_penalty = 0\n",
        "            if any(indicator in sentence_lower for indicator in ['et al', 'pp.', 'vol.', '[', ']', 'ibid', 'op. cit']):\n",
        "                citation_penalty = -2\n",
        "\n",
        "            total_score = keyword_overlap * 3 + position_score + length_score + info_bonus + citation_penalty\n",
        "\n",
        "            if keyword_overlap > 0 or info_bonus > 0:  # Must have some relevance\n",
        "                scored_sentences.append((total_score, sentence, i))\n",
        "\n",
        "        # Sort by score\n",
        "        scored_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "        if scored_sentences:\n",
        "            # Take top sentences\n",
        "            top_sentences = [sent[1] for sent in scored_sentences[:2]]\n",
        "            answer = '. '.join(top_sentences)\n",
        "            return self._clean_answer(answer)\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _is_reference_sentence(self, sentence: str) -> bool:\n",
        "        \"\"\"Check if sentence is likely a reference or citation\"\"\"\n",
        "        ref_indicators = [\n",
        "            'et al', 'pp.', 'vol.', 'ibid', 'op. cit',\n",
        "            '].', '[1', '[2', '[3', '[4', '[5',\n",
        "            'springer', 'elsevier', 'acm', 'ieee',\n",
        "            'proceedings', 'conference', 'journal'\n",
        "        ]\n",
        "        sentence_lower = sentence.lower()\n",
        "        return any(indicator in sentence_lower for indicator in ref_indicators)\n",
        "\n",
        "    def _generate_with_model(self, question: str, context: str) -> str:\n",
        "        \"\"\"Generate answer using the language model\"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"Answer the question based on the context provided. Be concise and accurate.\n",
        "\n",
        "Context: {context[:800]}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "            response = self.generator(\n",
        "                prompt,\n",
        "                max_new_tokens=80,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            full_response = response[0]['generated_text']\n",
        "            if \"Answer:\" in full_response:\n",
        "                answer = full_response.split(\"Answer:\")[-1].strip()\n",
        "                return self._clean_answer(answer)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Generation error: {str(e)}\")\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _simple_context_answer(self, context: str) -> str:\n",
        "        \"\"\"Simple fallback to return most relevant context\"\"\"\n",
        "        sentences = [s.strip() for s in context.split('.') if len(s.strip()) > 30]\n",
        "\n",
        "        # Filter out reference sentences\n",
        "        content_sentences = [s for s in sentences if not self._is_reference_sentence(s)]\n",
        "\n",
        "        if content_sentences:\n",
        "            return content_sentences[0] + \".\"\n",
        "        elif sentences:\n",
        "            return sentences[0] + \".\"\n",
        "        else:\n",
        "            return \"The relevant information is present in the document but requires more specific context to provide a clear answer.\"\n",
        "\n",
        "    def _clean_answer(self, answer: str) -> str:\n",
        "        \"\"\"Clean and format the answer\"\"\"\n",
        "        if not answer:\n",
        "            return \"No specific answer found in the provided context.\"\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        answer = ' '.join(answer.split())\n",
        "\n",
        "        # Remove common artifacts\n",
        "        answer = answer.replace('\\\\n', ' ')\n",
        "\n",
        "        # Ensure proper ending\n",
        "        if answer and not answer.endswith(('.', '!', '?')):\n",
        "            answer += '.'\n",
        "\n",
        "        # Limit length but try to end at sentence boundary\n",
        "        if len(answer) > 400:\n",
        "            sentences = answer.split('.')\n",
        "            if len(sentences) > 1:\n",
        "                # Take sentences that fit within limit\n",
        "                result = \"\"\n",
        "                for sent in sentences:\n",
        "                    if len(result + sent + \".\") <= 400:\n",
        "                        result += sent + \".\"\n",
        "                    else:\n",
        "                        break\n",
        "                answer = result if result else answer[:397] + \"...\"\n",
        "            else:\n",
        "                answer = answer[:397] + \"...\"\n",
        "\n",
        "        return answer"
      ],
      "metadata": {
        "id": "mOLQ-LoV4M6q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generation Pipeline:**\n",
        "\n",
        "*   Creates LLM prompt with question + context\n",
        "\n",
        "*   Uses DialoGPT-medium for generation\n",
        "\n",
        "*   Fallback to extractive QA if generation fails"
      ],
      "metadata": {
        "id": "ry0MyXxSOF1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 6: RAG System**\n",
        "**Main orchestrator class:**"
      ],
      "metadata": {
        "id": "F9KjDwa5OJYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Enhanced RAG system with multi-strategy approach\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.doc_processor = DocumentProcessor(chunk_size=400, chunk_overlap=50)  # Smaller chunks\n",
        "        self.vector_manager = VectorStoreManager()\n",
        "        self.answer_generator = AnswerGenerator()\n",
        "        self.is_ready = False\n",
        "        self.documents_info = {}\n",
        "\n",
        "    def setup(self, pdf_paths: List[str]) -> bool:\n",
        "        \"\"\"Setup with enhanced processing\"\"\"\n",
        "        print(\"ðŸš€ Setting up Enhanced RAG system...\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            print(\"Step 1: Loading PDFs...\")\n",
        "            documents = self.doc_processor.load_pdfs(pdf_paths)\n",
        "\n",
        "            if not documents:\n",
        "                print(\"âŒ No documents loaded!\")\n",
        "                return False\n",
        "\n",
        "            print(\"\\nStep 2: Creating text chunks...\")\n",
        "            chunks = self.doc_processor.create_chunks(documents)\n",
        "\n",
        "            if not chunks:\n",
        "                print(\"âŒ No chunks created!\")\n",
        "                return False\n",
        "\n",
        "            print(\"\\nStep 3: Creating vector embeddings...\")\n",
        "            if not self.vector_manager.create_vector_store(chunks):\n",
        "                return False\n",
        "\n",
        "            self.documents_info = {\n",
        "                'total_documents': len(documents),\n",
        "                'total_chunks': len(chunks),\n",
        "                'pdf_files': [os.path.basename(path) for path in pdf_paths],\n",
        "                'setup_complete': True\n",
        "            }\n",
        "\n",
        "            self.is_ready = True\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"ðŸŽ‰ Enhanced RAG System Ready!\")\n",
        "            print(f\"ðŸ“š Loaded: {len(documents)} pages from {len(pdf_paths)} PDFs\")\n",
        "            print(f\"ðŸ”ª Created: {len(chunks)} text chunks\")\n",
        "            print(f\"âš¡ Vector store ready with {self.vector_manager.vector_store.index.ntotal} embeddings\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Setup failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def ask_question(self, question: str, num_sources: int = 6) -> Dict[str, Any]:\n",
        "        \"\"\"Enhanced question answering with multiple strategies\"\"\"\n",
        "\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                'answer': \"âŒ System not ready. Please upload and process documents first.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        if not question.strip():\n",
        "            return {\n",
        "                'answer': \"âŒ Please provide a valid question.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"ðŸ” Processing question: '{question}'\")\n",
        "\n",
        "            # Use multiple search strategies\n",
        "            search_results = self._multi_strategy_search(question, num_sources)\n",
        "\n",
        "            if not search_results:\n",
        "                return {\n",
        "                    'answer': \"âŒ No relevant information found. Try rephrasing your question or check if the topic is covered in the uploaded documents.\",\n",
        "                    'sources': [],\n",
        "                    'confidence': 0.0\n",
        "                }\n",
        "\n",
        "            # Process results\n",
        "            context_parts = []\n",
        "            sources = []\n",
        "            similarity_scores = []\n",
        "\n",
        "            for doc, score in search_results:\n",
        "                similarity = max(0, 1 - score)  # Ensure non-negative\n",
        "                similarity_scores.append(similarity)\n",
        "                context_parts.append(doc.page_content)\n",
        "\n",
        "                source_info = {\n",
        "                    'file': doc.metadata.get('source_file', 'Unknown'),\n",
        "                    'page': doc.metadata.get('page_number', 'Unknown'),\n",
        "                    'chunk_id': doc.metadata.get('chunk_id', 'Unknown'),\n",
        "                    'similarity_score': round(similarity, 3),\n",
        "                    'content_preview': doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
        "                }\n",
        "                sources.append(source_info)\n",
        "\n",
        "            # Combine context more intelligently\n",
        "            combined_context = self._smart_context_combination(context_parts, question)\n",
        "\n",
        "            print(\"ðŸ¤– Generating answer...\")\n",
        "            answer = self.answer_generator.generate_answer(question, combined_context)\n",
        "\n",
        "            # Better confidence calculation\n",
        "            if similarity_scores:\n",
        "                # Use weighted average with higher weight for top results\n",
        "                weights = [1.0 / (i + 1) for i in range(len(similarity_scores))]\n",
        "                weighted_sum = sum(score * weight for score, weight in zip(similarity_scores, weights))\n",
        "                total_weight = sum(weights)\n",
        "                confidence = weighted_sum / total_weight\n",
        "                confidence = max(0.0, min(1.0, confidence))\n",
        "            else:\n",
        "                confidence = 0.0\n",
        "\n",
        "            result = {\n",
        "                'answer': answer,\n",
        "                'sources': sources,\n",
        "                'confidence': round(confidence, 3),\n",
        "                'context_length': len(combined_context),\n",
        "                'num_sources_used': len(sources)\n",
        "            }\n",
        "\n",
        "            print(f\"âœ… Answer generated (confidence: {result['confidence']})\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error processing question: {str(e)}\")\n",
        "            return {\n",
        "                'answer': f\"âŒ Error processing your question: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "    def _multi_strategy_search(self, question: str, k: int) -> List[tuple]:\n",
        "        \"\"\"Use multiple search strategies to find relevant content\"\"\"\n",
        "\n",
        "        all_results = []\n",
        "\n",
        "        # Strategy 1: Direct question search\n",
        "        results1 = self.vector_manager.search_similar(question, k=k//2)\n",
        "        all_results.extend(results1)\n",
        "\n",
        "        # Strategy 2: Enhanced query search\n",
        "        enhanced_query = self._enhance_query_advanced(question)\n",
        "        if enhanced_query != question:\n",
        "            results2 = self.vector_manager.search_similar(enhanced_query, k=k//2)\n",
        "            all_results.extend(results2)\n",
        "\n",
        "        # Strategy 3: Keyword-based search for definition questions\n",
        "        if any(word in question.lower() for word in ['what is', 'define', 'definition']):\n",
        "            keyword_query = self._extract_definition_keywords(question)\n",
        "            results3 = self.vector_manager.search_similar(keyword_query, k=k//3)\n",
        "            all_results.extend(results3)\n",
        "\n",
        "        # Remove duplicates and sort by score\n",
        "        seen_chunks = set()\n",
        "        unique_results = []\n",
        "\n",
        "        for doc, score in all_results:\n",
        "            chunk_id = doc.metadata.get('chunk_id', id(doc.page_content))\n",
        "            if chunk_id not in seen_chunks:\n",
        "                seen_chunks.add(chunk_id)\n",
        "                unique_results.append((doc, score))\n",
        "\n",
        "        # Sort by similarity score (lower distance = higher similarity)\n",
        "        unique_results.sort(key=lambda x: x[1])\n",
        "\n",
        "        return unique_results[:k]\n",
        "\n",
        "    def _enhance_query_advanced(self, question: str) -> str:\n",
        "        \"\"\"Advanced query enhancement\"\"\"\n",
        "        enhanced = question.lower()\n",
        "\n",
        "        # Add context-specific terms\n",
        "        if 'agent' in enhanced:\n",
        "            enhanced += \" artificial intelligence autonomous system behavior environment goal\"\n",
        "        elif any(word in enhanced for word in ['definition', 'define', 'what is']):\n",
        "            enhanced += \" definition concept meaning explanation\"\n",
        "        elif 'intelligence' in enhanced:\n",
        "            enhanced += \" AI artificial cognitive reasoning learning\"\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "    def _extract_definition_keywords(self, question: str) -> str:\n",
        "        \"\"\"Extract key terms for definition searches\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Extract the main term being defined\n",
        "        if 'what is' in question_lower:\n",
        "            term = question_lower.split('what is')[-1].strip('? .')\n",
        "        elif 'define' in question_lower:\n",
        "            term = question_lower.split('define')[-1].strip('? .')\n",
        "        else:\n",
        "            words = question.split()\n",
        "            term = ' '.join([w for w in words if w.lower() not in ['what', 'is', 'an', 'a', 'the']])\n",
        "\n",
        "        # Add related terms\n",
        "        if 'agent' in term:\n",
        "            return f\"{term} artificial intelligence autonomous system rational actor\"\n",
        "        else:\n",
        "            return term\n",
        "\n",
        "    def _smart_context_combination(self, context_parts: List[str], question: str) -> str:\n",
        "        \"\"\"Intelligently combine context parts\"\"\"\n",
        "        if not context_parts:\n",
        "            return \"\"\n",
        "\n",
        "        # Score context parts by relevance to question\n",
        "        question_words = set(question.lower().split())\n",
        "        scored_contexts = []\n",
        "\n",
        "        for context in context_parts:\n",
        "            context_words = set(context.lower().split())\n",
        "            overlap = len(question_words.intersection(context_words))\n",
        "\n",
        "            # Bonus for definition-like content\n",
        "            definition_bonus = 0\n",
        "            if any(phrase in context.lower() for phrase in ['is a', 'are a', 'refers to', 'defined as', 'means']):\n",
        "                definition_bonus = 2\n",
        "\n",
        "            score = overlap + definition_bonus\n",
        "            scored_contexts.append((score, context))\n",
        "\n",
        "        # Sort by relevance\n",
        "        scored_contexts.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "        # Combine top contexts\n",
        "        combined = \"\\n\\n\".join([context for _, context in scored_contexts])\n",
        "\n",
        "        # Limit total length\n",
        "        if len(combined) > 1500:\n",
        "            combined = combined[:1500] + \"...\"\n",
        "\n",
        "        return combined\n",
        "\n",
        "    def get_system_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system status\"\"\"\n",
        "        base_status = {\n",
        "            'is_ready': self.is_ready,\n",
        "            'documents_info': self.documents_info,\n",
        "            'vector_store_stats': self.vector_manager.get_stats()\n",
        "        }\n",
        "\n",
        "        if self.is_ready:\n",
        "            base_status['model_info'] = {\n",
        "                'embedding_model': self.vector_manager.model_name,\n",
        "                'generation_model': self.answer_generator.model_name,\n",
        "                'gpu_available': torch.cuda.is_available()\n",
        "            }\n",
        "\n",
        "        return base_status\n"
      ],
      "metadata": {
        "id": "s9Gkxxjf4Xlj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Workflow:**\n",
        "PDFs â†’ Chunks â†’ Embeddings â†’ Vector Store â†’ Query â†’ Answer"
      ],
      "metadata": {
        "id": "G5cCek5xONyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 7: File Helpers**\n",
        "**Colab-specific utilities:**"
      ],
      "metadata": {
        "id": "IVApw9b-OXrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def upload_pdf_files():\n",
        "    \"\"\"Helper function to upload PDF files in Colab\"\"\"\n",
        "    print(\"ðŸ“¤ Please upload your PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_paths = []\n",
        "    for filename, data in uploaded.items():\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Save file to current directory\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(data)\n",
        "            pdf_paths.append(filename)\n",
        "            print(f\"âœ… Saved: {filename}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Skipped non-PDF file: {filename}\")\n",
        "\n",
        "    return pdf_paths\n",
        "\n",
        "def setup_with_sample_data():\n",
        "    \"\"\"Setup system with sample data for demonstration\"\"\"\n",
        "    print(\"ðŸŽ¯ Setting up with sample data...\")\n",
        "    print(\"Note: In real usage, upload your own PDF files using upload_pdf_files()\")\n",
        "\n",
        "    # Create a sample document for demonstration\n",
        "    sample_content = \"\"\"\n",
        "    This is a sample AI research paper about Natural Language Processing.\n",
        "\n",
        "    Abstract: This paper presents a novel approach to question answering systems\n",
        "    using retrieval-augmented generation. Our method combines semantic search\n",
        "    with large language models to provide accurate and contextual answers.\n",
        "\n",
        "    Introduction: Question answering has been a fundamental challenge in NLP.\n",
        "    Recent advances in transformer models have shown promising results.\n",
        "\n",
        "    Methodology: We propose a hybrid approach that uses vector embeddings\n",
        "    for document retrieval and generative models for answer synthesis.\n",
        "\n",
        "    Results: Our system achieves 85% accuracy on benchmark datasets,\n",
        "    outperforming traditional keyword-based approaches by 20%.\n",
        "\n",
        "    Conclusion: The combination of retrieval and generation provides\n",
        "    superior performance for domain-specific question answering tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create sample PDF content (simplified)\n",
        "    from langchain.schema import Document\n",
        "\n",
        "    sample_doc = Document(\n",
        "        page_content=sample_content,\n",
        "        metadata={\n",
        "            'source_file': 'sample_paper.pdf',\n",
        "            'page_number': 1,\n",
        "            'total_pages': 1\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Process sample document\n",
        "    chunks = rag_system.doc_processor.create_chunks([sample_doc])\n",
        "\n",
        "    # Create vector store\n",
        "    if rag_system.vector_manager.create_vector_store(chunks):\n",
        "        rag_system.is_ready = True\n",
        "        rag_system.documents_info = {\n",
        "            'total_documents': 1,\n",
        "            'total_chunks': len(chunks),\n",
        "            'pdf_files': ['sample_paper.pdf'],\n",
        "            'setup_complete': True\n",
        "        }\n",
        "        print(\"âœ… Sample system ready for testing!\")\n",
        "        return True\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "CoVbpZAR4YsQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage Tips:**\n",
        "\n",
        "*   Drag-and-drop PDF upload support\n",
        "\n",
        "*   Sample data for quick testing"
      ],
      "metadata": {
        "id": "l8rwC4CDOf1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 8: Gradio UI**\n",
        "**Builds interactive interface:**"
      ],
      "metadata": {
        "id": "8AG6HkOHOk0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create an interactive web interface\"\"\"\n",
        "\n",
        "    def handle_file_upload(files):\n",
        "        if not files:\n",
        "            return \"âŒ Please upload at least one PDF file.\"\n",
        "\n",
        "        try:\n",
        "            pdf_paths = []\n",
        "            for file in files:\n",
        "                if file.name.endswith('.pdf'):\n",
        "                    pdf_paths.append(file.name)\n",
        "\n",
        "            if not pdf_paths:\n",
        "                return \"âŒ No valid PDF files found.\"\n",
        "\n",
        "            # Setup the RAG system\n",
        "            success = rag_system.setup(pdf_paths)\n",
        "\n",
        "            if success:\n",
        "                status = rag_system.get_system_status()\n",
        "                return f\"\"\"âœ… Setup Complete!\n",
        "\n",
        "ðŸ“Š System Status:\n",
        "â€¢ Documents loaded: {status['documents_info']['total_documents']}\n",
        "â€¢ Text chunks created: {status['documents_info']['total_chunks']}\n",
        "â€¢ Files processed: {', '.join(status['documents_info']['pdf_files'])}\n",
        "â€¢ Vector embeddings: {status['vector_store_stats']['total_vectors']}\n",
        "\n",
        "ðŸŽ¯ Ready to answer questions!\"\"\"\n",
        "            else:\n",
        "                return \"âŒ Setup failed. Please check your PDF files and try again.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"âŒ Error during setup: {str(e)}\"\n",
        "\n",
        "    def handle_question(question):\n",
        "        if not question.strip():\n",
        "            return \"âŒ Please enter a question.\", \"\"\n",
        "\n",
        "        result = rag_system.ask_question(question)\n",
        "\n",
        "        # Format answer\n",
        "        answer_text = f\"**Answer:** {result['answer']}\\n\\n\"\n",
        "        answer_text += f\"**Confidence:** {result['confidence']}/1.0\\n\\n\"\n",
        "\n",
        "        # Format sources\n",
        "        sources_text = \"**Sources:**\\n\\n\"\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            sources_text += f\"**{i}. {source['file']}** (Page {source['page']})\\n\"\n",
        "            sources_text += f\"   â€¢ Similarity: {source['similarity_score']}\\n\"\n",
        "            sources_text += f\"   â€¢ Preview: {source['content_preview']}\\n\\n\"\n",
        "\n",
        "        if not result['sources']:\n",
        "            sources_text = \"No sources found.\"\n",
        "\n",
        "        return answer_text, sources_text\n",
        "\n",
        "    def show_system_info():\n",
        "        status = rag_system.get_system_status()\n",
        "\n",
        "        if not status['is_ready']:\n",
        "            return \"System not ready. Please upload documents first.\"\n",
        "\n",
        "        info_text = f\"\"\"**System Information:**\n",
        "\n",
        "**Status:** {'âœ… Ready' if status['is_ready'] else 'âŒ Not Ready'}\n",
        "\n",
        "**Documents:**\n",
        "â€¢ Total documents: {status['documents_info']['total_documents']}\n",
        "â€¢ Total chunks: {status['documents_info']['total_chunks']}\n",
        "â€¢ Files: {', '.join(status['documents_info']['pdf_files'])}\n",
        "\n",
        "**Vector Store:**\n",
        "â€¢ Total vectors: {status['vector_store_stats']['total_vectors']}\n",
        "â€¢ Vector dimension: {status['vector_store_stats']['vector_dimension']}\n",
        "â€¢ Embedding model: {status['vector_store_stats']['model_name']}\n",
        "\n",
        "**Models:**\n",
        "â€¢ Generation model: {status.get('model_info', {}).get('generation_model', 'N/A')}\n",
        "â€¢ GPU available: {status.get('model_info', {}).get('gpu_available', False)}\n",
        "\"\"\"\n",
        "        return info_text\n",
        "\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(\n",
        "        title=\"RAG System - AI Research Papers QA\",\n",
        "        theme=gr.themes.Soft()\n",
        "    ) as interface:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ðŸ¤– RAG System: AI Research Papers Q&A\n",
        "\n",
        "        **Upload your AI research papers and ask questions about them!**\n",
        "\n",
        "        This system uses Retrieval-Augmented Generation to:\n",
        "        - ðŸ“– Process and understand your research papers\n",
        "        - ðŸ” Find relevant information for your questions\n",
        "        - ðŸ¤– Generate accurate answers with source citations\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"ðŸ“¤ Upload & Setup\"):\n",
        "            gr.Markdown(\"### Step 1: Upload Your PDF Research Papers\")\n",
        "\n",
        "            file_upload = gr.File(\n",
        "                label=\"Select PDF Files\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\"],\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            setup_btn = gr.Button(\"ðŸš€ Process Documents\", variant=\"primary\", size=\"lg\")\n",
        "            setup_output = gr.Textbox(\n",
        "                label=\"Setup Status\",\n",
        "                lines=10,\n",
        "                interactive=False,\n",
        "                placeholder=\"Upload PDFs and click 'Process Documents' to begin...\"\n",
        "            )\n",
        "\n",
        "            # Sample data button for demo\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### Or Try with Sample Data\")\n",
        "            sample_btn = gr.Button(\"ðŸŽ¯ Use Sample Data (Demo)\", variant=\"secondary\")\n",
        "\n",
        "            def setup_sample():\n",
        "                success = setup_with_sample_data()\n",
        "                if success:\n",
        "                    status = rag_system.get_system_status()\n",
        "                    return f\"\"\"âœ… Sample System Ready!\n",
        "\n",
        "This is a demo with sample AI research paper content.\n",
        "You can now ask questions like:\n",
        "â€¢ \"What is the main contribution of this paper?\"\n",
        "â€¢ \"What methodology was used?\"\n",
        "â€¢ \"What were the results?\"\n",
        "\n",
        "ðŸ“Š System loaded with {status['documents_info']['total_chunks']} text chunks.\"\"\"\n",
        "                else:\n",
        "                    return \"âŒ Failed to setup sample data.\"\n",
        "\n",
        "            sample_btn.click(setup_sample, outputs=[setup_output])\n",
        "            setup_btn.click(handle_file_upload, inputs=[file_upload], outputs=[setup_output])\n",
        "\n",
        "        with gr.Tab(\"â“ Ask Questions\"):\n",
        "            gr.Markdown(\"### Ask Questions About Your Research Papers\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=4):\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"Your Question\",\n",
        "                        placeholder=\"e.g., What are the main contributions of this research?\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                with gr.Column(scale=1):\n",
        "                    ask_btn = gr.Button(\"ðŸ” Get Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    answer_output = gr.Textbox(\n",
        "                        label=\"Answer\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    sources_output = gr.Textbox(\n",
        "                        label=\"Sources & Citations\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            # Sample questions\n",
        "            gr.Markdown(\"### ðŸ’¡ Sample Questions to Try:\")\n",
        "            sample_questions = [\n",
        "                \"What are the main contributions of this research?\",\n",
        "                \"What methodology was used in this study?\",\n",
        "                \"What are the key findings and results?\",\n",
        "                \"What are the limitations mentioned?\",\n",
        "                \"What future work is suggested?\"\n",
        "            ]\n",
        "\n",
        "            for q in sample_questions:\n",
        "                sample_q_btn = gr.Button(f\"ðŸ“ {q}\", variant=\"secondary\", size=\"sm\")\n",
        "                sample_q_btn.click(lambda x=q: x, outputs=[question_input])\n",
        "\n",
        "            ask_btn.click(\n",
        "                handle_question,\n",
        "                inputs=[question_input],\n",
        "                outputs=[answer_output, sources_output]\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"â„¹ï¸ System Info\"):\n",
        "            gr.Markdown(\"### System Status and Information\")\n",
        "\n",
        "            info_btn = gr.Button(\"ðŸ”„ Refresh System Info\", variant=\"primary\")\n",
        "            info_output = gr.Textbox(\n",
        "                label=\"System Information\",\n",
        "                lines=15,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            info_btn.click(show_system_info, outputs=[info_output])\n",
        "\n",
        "            # Auto-load info on tab open\n",
        "            interface.load(show_system_info, outputs=[info_output])\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create the RAG system instance globally\n",
        "rag_system = RAGSystem()\n",
        "\n",
        "# Create the interface\n",
        "print(\"\\nðŸŽ¨ Creating Gradio Interface...\")\n",
        "interface = create_gradio_interface()\n",
        "print(\"âœ… Interface ready!\")"
      ],
      "metadata": {
        "id": "2mwlIx2d4pfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469b2984-d7f1-4359-bd57-da6609627b91"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– Loading language model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Language model loaded!\n",
            "\n",
            "ðŸŽ¨ Creating Gradio Interface...\n",
            "âœ… Interface ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interface Features:**\n",
        "\n",
        "*   Document upload/status panel\n",
        "\n",
        "*   Q&A with source citations\n",
        "\n",
        "*   System diagnostics view"
      ],
      "metadata": {
        "id": "oLAcDrFkOrmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 9: Runtime Controls**\n",
        "**Launch commands:**"
      ],
      "metadata": {
        "id": "FEV5wiaSOu0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_system():\n",
        "    \"\"\"Launch the complete system\"\"\"\n",
        "    print(\"ðŸš€ Launching RAG System...\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"ðŸ“‹ Instructions:\")\n",
        "    print(\"1. Run this cell to launch the web interface\")\n",
        "    print(\"2. Upload your PDF research papers in the 'Upload & Setup' tab\")\n",
        "    print(\"3. Wait for processing to complete\")\n",
        "    print(\"4. Go to 'Ask Questions' tab and start asking!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Launch interface\n",
        "    interface.launch(\n",
        "        share=True,  # Creates a public shareable link\n",
        "        debug=True,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "# Quick test function\n",
        "def quick_test():\n",
        "    \"\"\"Quick test with sample data\"\"\"\n",
        "    print(\"ðŸ§ª Quick Test Mode\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Setup with sample data\n",
        "    if setup_with_sample_data():\n",
        "        print(\"\\nâœ… Sample system ready!\")\n",
        "\n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            \"What is this paper about?\",\n",
        "            \"What methodology was used?\",\n",
        "            \"What were the main results?\"\n",
        "        ]\n",
        "\n",
        "        for question in test_questions:\n",
        "            print(f\"\\nâ“ Question: {question}\")\n",
        "            result = rag_system.ask_question(question, num_sources=2)\n",
        "            print(f\"âœ… Answer: {result['answer'][:200]}...\")\n",
        "            print(f\"ðŸ“š Sources: {len(result['sources'])} found\")\n",
        "            print(f\"ðŸŽ¯ Confidence: {result['confidence']}\")\n",
        "    else:\n",
        "        print(\"âŒ Test setup failed\")"
      ],
      "metadata": {
        "id": "ttceAWcx_UWH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two Usage Modes:**\n",
        "\n",
        "*   Full web interface (run_system())\n",
        "\n",
        "*   CLI-style testing (quick_test())"
      ],
      "metadata": {
        "id": "GhZIl7VbO0RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_system()"
      ],
      "metadata": {
        "id": "mUlgm2hB5Hov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a9ecbb8-cbdd-4131-8590-be3e546632b8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Launching RAG System...\n",
            "============================================================\n",
            "ðŸ“‹ Instructions:\n",
            "1. Run this cell to launch the web interface\n",
            "2. Upload your PDF research papers in the 'Upload & Setup' tab\n",
            "3. Wait for processing to complete\n",
            "4. Go to 'Ask Questions' tab and start asking!\n",
            "============================================================\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://62dd8a6aa95ffa2578.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://62dd8a6aa95ffa2578.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Setting up Enhanced RAG system...\n",
            "==================================================\n",
            "Step 1: Loading PDFs...\n",
            "ðŸ“– Loading: agentic ai RD paper.pdf\n",
            "âœ… Loaded 26 pages from agentic ai RD paper.pdf\n",
            "\n",
            "Step 2: Creating text chunks...\n",
            "ðŸ”ª Creating text chunks...\n",
            "âœ… Created 286 chunks\n",
            "\n",
            "Step 3: Creating vector embeddings...\n",
            "âš¡ Creating vector embeddings...\n",
            "ðŸ“Š Processing 286 chunks...\n",
            "ðŸ“Š Using 285 meaningful chunks...\n",
            "âœ… Vector store created successfully!\n",
            "ðŸ“ˆ Vector dimension: 384\n",
            "ðŸ“š Total vectors: 285\n",
            "\n",
            "==================================================\n",
            "ðŸŽ‰ Enhanced RAG System Ready!\n",
            "ðŸ“š Loaded: 26 pages from 1 PDFs\n",
            "ðŸ”ª Created: 286 text chunks\n",
            "âš¡ Vector store ready with 285 embeddings\n",
            "==================================================\n",
            "ðŸ” Processing question: 'What is an agent?'\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 2 relevant chunks for query\n",
            "ðŸ¤– Generating answer...\n",
            "âœ… Answer generated (confidence: 0.3050000071525574)\n",
            "ðŸ” Processing question: 'What are the characteristics of an agent?'\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ¤– Generating answer...\n",
            "âœ… Answer generated (confidence: 0.31700000166893005)\n",
            "ðŸ” Processing question: 'define agentic ai\n",
            "'\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 2 relevant chunks for query\n",
            "ðŸ¤– Generating answer...\n",
            "âœ… Answer generated (confidence: 0.3050000071525574)\n",
            "ðŸ” Processing question: 'define agentic ai\n",
            "'\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 2 relevant chunks for query\n",
            "ðŸ¤– Generating answer...\n",
            "âœ… Answer generated (confidence: 0.3050000071525574)\n",
            "ðŸ” Processing question: 'what is ai gaents\n",
            "'\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 2 relevant chunks for query\n",
            "ðŸ¤– Generating answer...\n",
            "âœ… Answer generated (confidence: 0.17000000178813934)\n",
            "ðŸ” Processing question: 'what is ai agents\n",
            "'\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 3 relevant chunks for query\n",
            "ðŸ” Found 2 relevant chunks for query\n",
            "ðŸ¤– Generating answer...\n",
            "âœ… Answer generated (confidence: 0.35199999809265137)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://62dd8a6aa95ffa2578.gradio.live\n"
          ]
        }
      ]
    }
  ]
}